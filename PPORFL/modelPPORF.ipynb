{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a9bac6",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO)\n",
    "---\n",
    "\n",
    "### **Reward (รางวัล)**\n",
    "\n",
    "**คำอธิบายที่ปรับปรุงแล้ว:**\n",
    "**Reward** คือ **สัญญาณตอบรับทันที** ที่ Agent ได้รับจากสิ่งแวดล้อม (Environment) หลังจากทำ Action หนึ่งๆ ใน State ณ เวลานั้น มันเป็นตัวเลขที่บอกว่า Action ที่เพิ่งทำไปนั้น \"ดี\" หรือ \"ไม่ดี\" ณ **วินาทีนั้น**\n",
    "\n",
    "* **ตัวอย่าง CartPole:**\n",
    "    * ได้ `+1` reward สำหรับทุกๆ step ที่ไม้ยังไม่ล้ม\n",
    "    * จบเกม (ไม้ล้ม) reward เป็น `0` และไม่ได้ไปต่อ\n",
    "\n",
    "Reward เป็นเหมือน \"เงินเดือน\" ที่ได้ทันที แต่ยังไม่ได้บอกว่าการตัดสินใจทำงานนี้จะส่งผลดีต่อ \"ความมั่งคั่งในระยะยาว\" หรือไม่\n",
    "\n",
    "---\n",
    "\n",
    "### **Value (ค่า Value)**\n",
    "\n",
    "**คำอธิบายที่ปรับปรุงแล้ว:**\n",
    "**Value** หรือ **State-Value Function ($V(s)$)** คือ **การประเมินผลตอบแทนรวมในอนาคต (Total Future Reward)** ที่คาดว่าจะได้รับ ถ้าเริ่มต้นจาก State นั้นๆ แล้วทำตาม Policy (นโยบายการตัดสินใจ) ปัจจุบันต่อไปจนจบเกม\n",
    "\n",
    "พูดง่ายๆ คือ มันตอบคำถามว่า \"การอยู่ใน State นี้ มันดีแค่ไหนในระยะยาว?\"\n",
    "\n",
    "* **เปรียบเทียบ:** ถ้า Reward คือ \"เงินเดือน\" ที่ได้ทันที Value ก็เปรียบเสมือน \"ศักยภาพในการสร้างรายได้ในอนาคต\" จากตำแหน่งงานปัจจุบัน\n",
    "* **ในโค้ด:** `self.value` head ของโมเดลพยายามเรียนรู้ที่จะทำนายค่านี้ มันคือค่าที่ \"คาดไว้\" (Expected Value) ว่าอนาคตจะเป็นอย่างไรจากจุดนี้\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantage (ค่า Advantage)**\n",
    "\n",
    "**คำอธิบายที่ปรับปรุงแล้ว:**\n",
    "**Advantage Function ($A(s,a)$)** เป็นตัวชี้วัดว่า Action หนึ่งๆ ที่เลือกทำใน State นั้นๆ **ดีกว่าค่าเฉลี่ย** ที่คาดไว้จาก State นั้นมากแค่ไหน\n",
    "\n",
    "สูตรแนวคิดคือ: $A(s, a) = Q(s, a) - V(s)$\n",
    "\n",
    "* $Q(s, a)$: คือค่า Value ที่คาดว่าจะได้รับถ้าเลือก Action `a` ใน State `s`\n",
    "* $V(s)$: คือค่า Value เฉลี่ยของ State `s` (จากการทำทุก Action ที่เป็นไปได้ตาม Policy)\n",
    "\n",
    "**อธิบายง่ายๆ:**\n",
    "* **Advantage > 0 (ค่าบวก):** Action ที่เราทำนั้นให้ผลลัพธ์ดีกว่าที่คาดไว้ (เป็น \"Surprise\" ที่ดี) ควรทำ Action นี้บ่อยขึ้น\n",
    "* **Advantage < 0 (ค่าลบ):** Action ที่เราทำนั้นให้ผลลัพธ์แย่กว่าที่คาดไว้ (เป็น \"Surprise\" ที่น่าผิดหวัง) ควรทำ Action นี้น้อยลง\n",
    "\n",
    "ในโค้ดนี้ใช้เทคนิคที่เรียกว่า **Generalized Advantage Estimation (GAE)** ซึ่งเป็นวิธีคำนวณ Advantage ที่ซับซ้อนขึ้นเล็กน้อย เพื่อลดความคลาดเคลื่อนและทำให้การเรียนรู้เสถียรขึ้น\n",
    "\n",
    "---\n",
    "\n",
    "### **Return (ค่า Return)**\n",
    "\n",
    "**คำอธิบายที่ปรับปรุงแล้ว:**\n",
    "**Return ($G_t$)** คือ **ผลรวมของ Reward ที่เกิดขึ้นจริง** นับตั้งแต่เวลา $t$ ไปจนจบ Episode (มีการคิดลดทอนค่าตามเวลาด้วย `gamma`) มันคือ \"ผลลัพธ์จริงๆ\" ที่เกิดขึ้น\n",
    "\n",
    "**ลำดับที่ถูกต้องคือ:**\n",
    "1.  Agent เล่นไปจนจบ Episode หรือเก็บข้อมูลครบ `STEPS_PER_UPDATE`\n",
    "2.  เรามองย้อนกลับไป และคำนวณ **Return ($G_t$)** ที่เกิดขึ้นจริงในแต่ละ Step\n",
    "3.  เราใช้ Return ($G_t$) และ Value ที่โมเดลทำนายไว้ ($V(s)$) มาคำนวณ **Advantage**\n",
    "\n",
    "ดังนั้น **Return** คือเป้าหมาย (Target) ที่เราจะใช้สอน Value Function ให้ทำนายได้แม่นยำขึ้น ไม่ใช่ผลลัพธ์ที่ได้หลังจากการคำนวณ Advantage ครับ\n",
    "\n",
    "ในโค้ดนี้ `returns = advantages + values[:-1]` คือการสร้าง \"Target\" สำหรับการอัปเดต Value Function โดยใช้ค่า Advantage ที่คำนวณจาก GAE ซึ่งเป็นเทคนิคที่เสถียรกว่าการใช้ Return ดิบๆ\n",
    "\n",
    "---\n",
    "\n",
    "### **Logprobs (ค่า Log Probs)**\n",
    "\n",
    "**คำอธิบายที่ปรับปรุงแล้ว:**\n",
    "**Logprobs** คือ **ค่า Logarithm ของความน่าจะเป็น (Probability)** ที่ Policy จะเลือก Action หนึ่งๆ ใน State นั้นๆ\n",
    "\n",
    "* **ทำไมต้องใช้ Log?**\n",
    "    1.  **ความเสถียรทางตัวเลข (Numerical Stability):** ความน่าจะเป็นเป็นตัวเลขระหว่าง 0-1 การคูณเลขทศนิยมเล็กๆ จำนวนมากเข้าด้วยกันอาจทำให้ค่าเข้าใกล้ 0 จนเกิดปัญหา (underflow) การเปลี่ยนเป็น Log จะทำให้การคูณกลายเป็นการบวก ซึ่งจัดการได้ง่ายกว่า\n",
    "    2.  **ความสะดวกในการคำนวณ:** ในการทำ Optimization การหาอนุพันธ์ (derivative) ของผลบวกนั้นง่ายกว่าผลคูณ\n",
    "\n",
    "ค่า Logprobs จึงเป็นตัวแทน \"ความมั่นใจ\" ของโมเดลในการเลือก Action นั้นๆ ในรูปแบบ Log scale\n",
    "\n",
    "---\n",
    "\n",
    "### **Ratio (ค่า Ratio)**\n",
    "\n",
    "**คำอธิบายที่ปรับปรุงแล้ว:**\n",
    "**Ratio** คือ **อัตราส่วนความน่าจะเป็น** ระหว่าง Policy **ใหม่** (ที่กำลังจะอัปเดต) กับ Policy **เก่า** (ที่ใช้เก็บข้อมูล) ในการเลือก Action เดียวกัน\n",
    "\n",
    "$$\\text{ratio} = \\frac{\\pi_{\\text{new}}(a|s)}{\\pi_{\\text{old}}(a|s)}$$\n",
    "\n",
    "* **Ratio > 1:** Policy ใหม่มีความมั่นใจที่จะเลือก Action นี้ **มากกว่า** Policy เก่า\n",
    "* **Ratio < 1:** Policy ใหม่มีความมั่นใจที่จะเลือก Action นี้ **น้อยกว่า** Policy เก่า\n",
    "* **ในโค้ด:** คำนวณจาก `torch.exp(mb_logprobs - mb_old_logprobs)` ซึ่งเท่ากับสมการข้างบน เพราะ $e^{(\\log(a) - \\log(b))} = e^{\\log(a/b)} = a/b$\n",
    "\n",
    "Ratio เป็นหัวใจของ PPO ที่บอกเราว่าการอัปเดตครั้งนี้จะเปลี่ยน Policy ไปมากน้อยแค่ไหน\n",
    "\n",
    "---\n",
    "\n",
    "### **Surrogate Objectives และ Policy Loss**\n",
    "\n",
    "**คำอธิบายที่ปรับปรุงแล้ว:**\n",
    "เป้าหมายหลักของเราคือการทำให้ผลตอบแทนสูงขึ้น โดยการปรับ Policy ไปในทิศทางที่ให้ Advantage เป็นบวก\n",
    "\n",
    "* `surrogate1 = ratio * mb_adv`: นี่คือเป้าหมายพื้นฐาน ถ้า `mb_adv` เป็นบวก (ดี) เราก็อยากเพิ่ม `ratio` (ทำ Action นี้บ่อยขึ้น) ถ้า `mb_adv` เป็นลบ (แย่) เราก็อยากลด `ratio` (ทำ Action นี้น้อยลง) **แต่...** การปรับ `ratio` มากเกินไปอาจทำให้การเรียนรู้พังได้!\n",
    "\n",
    "* `surrogate2 = torch.clamp(ratio, 1.0 - CLIP_EPS, 1.0 + CLIP_EPS) * mb_adv`: นี่คือเวอร์ชัน \"ปลอดภัย\" ของเป้าหมาย มันคือการ **Clip** หรือ **จำกัดขอบเขตของ `ratio`** ไม่ให้เปลี่ยนแปลงไปจากเดิม (คือ 1.0) มากเกินกว่าค่า `CLIP_EPS` (เช่น 0.2) เพื่อป้องกันการอัปเดตที่ก้าวกระโดดเกินไป\n",
    "\n",
    "* `policy_loss = -torch.min(surrogate1, surrogate2).mean()`: PPO เลือกใช้ค่าที่ **น้อยกว่า** ระหว่าง `surrogate1` และ `surrogate2` เสมอ นี่คือหลักการ \"มองโลกในแง่ร้าย\" (Pessimistic) เพื่อให้การอัปเดตเป็นไปอย่างระมัดระวังที่สุด และการใส่เครื่องหมาย **ลบ (`-`)** ก็เพื่อเปลี่ยนปัญหาจากการ Maximization (ทำให้รางวัลสูงสุด) ไปเป็น Minimization (ทำให้ Loss ต่ำสุด) ซึ่งเป็นสิ่งที่ Optimizer ทั่วไปทำ\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Value Loss**\n",
    "\n",
    "**คำอธิบายที่ปรับปรุงแล้ว:**\n",
    "`value_loss = (mb_returns - values).pow(2).mean()`: ส่วนนี้ตรงไปตรงมา คือการวัดว่า Value ที่โมเดลทำนาย (`values`) แตกต่างจาก \"เป้าหมาย\" ที่เราคำนวณไว้ (`mb_returns`) มากแค่ไหน (วัดด้วย Mean Squared Error) เป้าหมายคือทำให้ Value Loss เข้าใกล้ 0 ที่สุด ซึ่งหมายความว่า \"นักวิจารณ์\" (Critic) ของเราทำนายอนาคตได้แม่นยำขึ้นเรื่อยๆ\n",
    "\n",
    "---\n",
    "\n",
    "### **Entropy (ค่า Entropy)**\n",
    "\n",
    "**คำอธิบายที่ปรับปรุงแล้ว:**\n",
    "**Entropy** คือ **ค่าวัดความไม่แน่นอน** หรือความ \"สุ่ม\" ของ Policy\n",
    "* **Entropy สูง:** Policy มีความลังเล กระจายความน่าจะเป็นไปให้หลายๆ Action (คล้ายๆ กัน) ซึ่งดีต่อการ **สำรวจ (Exploration)** ในช่วงแรกๆ\n",
    "* **Entropy ต่ำ:** Policy มีความมั่นใจสูง ความน่าจะเป็นจะเทไปที่ Action ใด Action หนึ่งอย่างชัดเจน (ดีต่อการ **ใช้ประโยชน์ (Exploitation)** เมื่อเรียนรู้ไปสักพักแล้ว)\n",
    "\n",
    "---\n",
    "\n",
    "### **Total Loss (Loss รวม)**\n",
    "\n",
    "**คำอธิบายที่ปรับปรุงแล้ว:**\n",
    "`loss = policy_loss + VALUE_COEF * value_loss - ENTROPY_COEF * entropy`\n",
    "Loss รวมคือการนำเป้าหมายทั้ง 3 ส่วนมารวมกัน เพื่อหาจุดสมดุลที่ดีที่สุดในการอัปเดตโมเดล:\n",
    "\n",
    "1.  **`policy_loss` (เป้าหมายหลัก):** ปรับปรุง Policy ให้ดีขึ้นอย่างระมัดระวัง\n",
    "2.  **`VALUE_COEF * value_loss` (เป้าหมายรอง):** ปรับปรุงความแม่นยำของตัวประเมินค่า (Critic)\n",
    "3.  **`- ENTROPY_COEF * entropy` (ตัวช่วย):** ส่งเสริมการสำรวจ (Exploration) โดยการ \"ลงโทษ\" Policy ที่มั่นใจในตัวเองเร็วเกินไป (Entropy ต่ำ) เราใส่เครื่องหมายลบ เพราะเราต้องการ **Maximize** Entropy แต่ Optimizer ทำได้แค่ **Minimize** Loss ดังนั้นการ Minimize \"ลบ Entropy\" จึงเท่ากับการ Maximize Entropy นั่นเอง\n",
    "\n",
    "`VALUE_COEF` และ `ENTROPY_COEF` คือค่าน้ำหนักที่บอกว่าเราให้ความสำคัญกับแต่ละเป้าหมายมากน้อยแค่ไหน"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33959861",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73935bf0",
   "metadata": {},
   "source": [
    "# ppo_cartpole.py\n",
    "import gym\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "SEED = 123\n",
    "GAMMA = 0.99\n",
    "LAM = 0.95                # GAE lambda\n",
    "CLIP_EPS = 0.2\n",
    "LR = 3e-4\n",
    "ENTROPY_COEF = 0.01\n",
    "VALUE_COEF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "UPDATE_EPOCHS = 10       # how many epochs per update\n",
    "MINI_BATCH_SIZE = 64\n",
    "STEPS_PER_UPDATE = 2048  # collect this many steps then update\n",
    "TOTAL_UPDATES = 2000     # outer loop iterations\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Utils / Seed ---\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --- Actor-Critic Network ---\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_size=64):\n",
    "        super().__init__()\n",
    "        # common body\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # policy head\n",
    "        self.policy = nn.Linear(hidden_size, act_dim)\n",
    "        # value head\n",
    "        self.value = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        logits = self.policy(x)\n",
    "        value = self.value(x).squeeze(-1)\n",
    "        return logits, value\n",
    "\n",
    "    def act(self, obs):\n",
    "        logits, value = self.forward(obs)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        logp = dist.log_prob(action)\n",
    "        return action.item(), logp, value, dist.entropy()\n",
    "\n",
    "    def get_logprob_value(self, obs, act):\n",
    "        logits, value = self.forward(obs)\n",
    "        dist = Categorical(logits=logits)\n",
    "        logp = dist.log_prob(act)\n",
    "        entropy = dist.entropy()\n",
    "        return logp, value, entropy\n",
    "\n",
    "# --- Rollout Buffer (on-policy) ---\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.logprobs = []\n",
    "        self.values = []\n",
    "\n",
    "    def add(self, obs, action, reward, done, logprob, value):\n",
    "        self.obs.append(obs)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.logprobs.append(logprob)\n",
    "        self.values.append(value)\n",
    "\n",
    "    def clear(self):\n",
    "        self.__init__()\n",
    "\n",
    "    def compute_returns_and_advantages(self, last_value, gamma=GAMMA, lam=LAM):\n",
    "        # convert to numpy arrays\n",
    "        rewards = np.array(self.rewards, dtype=np.float32)\n",
    "        values = np.array(self.values + [last_value], dtype=np.float32)\n",
    "        dones = np.array(self.dones, dtype=np.float32)\n",
    "        deltas = rewards + gamma * values[1:] * (1 - dones) - values[:-1] \n",
    "        advantages = np.zeros_like(deltas)\n",
    "        adv = 0.0\n",
    "        for t in reversed(range(len(deltas))):\n",
    "            adv = deltas[t] + gamma * lam * (1 - dones[t]) * adv\n",
    "            advantages[t] = adv\n",
    "        returns = advantages + values[:-1]\n",
    "        # convert to tensors\n",
    "        obs = torch.tensor(np.array(self.obs), dtype=torch.float32, device=DEVICE)\n",
    "        actions = torch.tensor(self.actions, dtype=torch.long, device=DEVICE)\n",
    "        logprobs = torch.stack(self.logprobs).to(DEVICE).detach()\n",
    "        values = torch.tensor(self.values, dtype=torch.float32, device=DEVICE)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32, device=DEVICE)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32, device=DEVICE)\n",
    "        # normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        return obs, actions, logprobs, values, returns, advantages\n",
    "\n",
    "# --- PPO training function ---\n",
    "def ppo_train():\n",
    "    env = gym.make(ENV_NAME)\n",
    "    # env.seed(SEED)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.n\n",
    "\n",
    "    print(f\"Observation space: {obs_dim}, Action space: {act_dim}\")\n",
    "\n",
    "    model = ActorCritic(obs_dim, act_dim).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    buffer = RolloutBuffer()\n",
    "    episode_rewards = deque(maxlen=100)\n",
    "\n",
    "    obs = env.reset()[0]\n",
    "    ep_reward = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    for update in range(1, TOTAL_UPDATES + 1):\n",
    "        # collect rollouts\n",
    "        # print(f\"=== Update {update} ===\")\n",
    "        for step in range(STEPS_PER_UPDATE):\n",
    "            # print(\"obs:\", obs)\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "            # print(\"step:\", step)\n",
    "            # print(obs_tensor)\n",
    "            # print(obs_tensor.shape)\n",
    "            with torch.no_grad():\n",
    "                logits, value = model.forward(obs_tensor)\n",
    "                # print(\"logits:\", logits)\n",
    "                # print(\"value:\", value)\n",
    "                dist = Categorical(logits=logits)\n",
    "                # print(\"dist:\", dist)\n",
    "                action = dist.sample().cpu().numpy()[0]\n",
    "                # print(\"action:\", action)\n",
    "                logp = dist.log_prob(torch.tensor(action, device=DEVICE))\n",
    "                # print(\"logp:\", logp)\n",
    "            next_obs, reward, done, info, _ = env.step(int(action))\n",
    "            # print(\"next_obs:\", next_obs)\n",
    "            # print(\"reward:\", reward)\n",
    "            # print(\"done:\", done)\n",
    "            # print(\"info:\", info)\n",
    "            buffer.add(obs, action, reward, done, logp, value.item())\n",
    "            obs = next_obs\n",
    "            ep_reward += reward\n",
    "            total_steps += 1\n",
    "\n",
    "            if done:\n",
    "                # print(f\"Episode finished after {step+1} steps.\")\n",
    "                episode_rewards.append(ep_reward)\n",
    "                # print(f\"Episode reward: {ep_reward}\")\n",
    "                obs = env.reset()[0]\n",
    "                ep_reward = 0\n",
    "        # print(\"Episode rewards (last 10):\", list(episode_rewards)[-10:])\n",
    "        # print(f\"Collected {STEPS_PER_UPDATE} steps.\")\n",
    "\n",
    "        # compute last value for bootstrap\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            _, last_value = model.forward(obs_tensor)\n",
    "            last_value = last_value.item()\n",
    "\n",
    "        # prepare training data\n",
    "        obs_b, actions_b, old_logprobs_b, values_b, returns_b, adv_b = buffer.compute_returns_and_advantages(last_value)\n",
    "        buffer.clear()\n",
    "\n",
    "        # PPO update: multiple epochs, minibatches\n",
    "        dataset_size = obs_b.size(0)\n",
    "        for epoch in range(UPDATE_EPOCHS):\n",
    "            # generate permutation for minibatches\n",
    "            perm = torch.randperm(dataset_size)\n",
    "            for start in range(0, dataset_size, MINI_BATCH_SIZE):\n",
    "                idx = perm[start:start + MINI_BATCH_SIZE]\n",
    "                mb_obs = obs_b[idx]\n",
    "                mb_actions = actions_b[idx]\n",
    "                mb_old_logprobs = old_logprobs_b[idx]\n",
    "                mb_returns = returns_b[idx]\n",
    "                mb_adv = adv_b[idx]\n",
    "\n",
    "                # current policy\n",
    "                logits, values = model.forward(mb_obs)\n",
    "                dist = Categorical(logits=logits)\n",
    "                mb_logprobs = dist.log_prob(mb_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                # ratio for clipping\n",
    "                ratio = torch.exp(mb_logprobs - mb_old_logprobs)\n",
    "\n",
    "                # clipped surrogate objective\n",
    "                surrogate1 = ratio * mb_adv #\n",
    "                surrogate2 = torch.clamp(ratio, 1.0 - CLIP_EPS, 1.0 + CLIP_EPS) * mb_adv\n",
    "                policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "                # value loss (MSE)\n",
    "                value_loss = (mb_returns - values).pow(2).mean()\n",
    "\n",
    "                # total loss\n",
    "                loss = policy_loss + VALUE_COEF * value_loss - ENTROPY_COEF * entropy\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "                optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        if len(episode_rewards) > 0:\n",
    "            avg_reward = sum(episode_rewards) / len(episode_rewards)\n",
    "        else:\n",
    "            avg_reward = 0.0\n",
    "        print(f\"Update {update:4d}  Steps {total_steps:7d}  AvgReward(100) {avg_reward:6.2f}\")\n",
    "\n",
    "        # (optional) early stop if environment solved\n",
    "        if avg_reward >= 475.0 and len(episode_rewards) >= 100:\n",
    "            print(\"Environment solved!\")\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    trained_model = ppo_train()\n",
    "    print(\"Training done in {:.2f} sec\".format(time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
