# ppo_cartpole.py
import gym
import math
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import numpy as np
from collections import deque

# --- Hyperparameters ---
ENV_NAME = "CartPole-v1"
SEED = 123
GAMMA = 0.99
LAM = 0.95                # GAE lambda
CLIP_EPS = 0.2
LR = 3e-4
ENTROPY_COEF = 0.01
VALUE_COEF = 0.5
MAX_GRAD_NORM = 0.5
UPDATE_EPOCHS = 10       # how many epochs per update
MINI_BATCH_SIZE = 64
STEPS_PER_UPDATE = 2048  # collect this many steps then update
TOTAL_UPDATES = 2000     # outer loop iterations
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- Utils / Seed ---
torch.manual_seed(SEED)
np.random.seed(SEED)

# --- Actor-Critic Network ---
class ActorCritic(nn.Module):
    def __init__(self, obs_dim, act_dim, hidden_size=64):
        super().__init__()
        # common body
        self.shared = nn.Sequential(
            nn.Linear(obs_dim, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, hidden_size),
            nn.Tanh()
        )
        # policy head
        self.policy = nn.Linear(hidden_size, act_dim)
        # value head
        self.value = nn.Linear(hidden_size, 1)

    def forward(self, x):
        x = self.shared(x)
        logits = self.policy(x)
        value = self.value(x).squeeze(-1)
        return logits, value

    def act(self, obs):
        logits, value = self.forward(obs)
        dist = Categorical(logits=logits)
        action = dist.sample()
        logp = dist.log_prob(action)
        return action.item(), logp, value, dist.entropy()

    def get_logprob_value(self, obs, act):
        logits, value = self.forward(obs)
        dist = Categorical(logits=logits)
        logp = dist.log_prob(act)
        entropy = dist.entropy()
        return logp, value, entropy

# --- Rollout Buffer (on-policy) ---
class RolloutBuffer:
    def __init__(self):
        self.obs = []
        self.actions = []
        self.rewards = []
        self.dones = []
        self.logprobs = []
        self.values = []

    def add(self, obs, action, reward, done, logprob, value):
        self.obs.append(obs)
        self.actions.append(action)
        self.rewards.append(reward)
        self.dones.append(done)
        self.logprobs.append(logprob)
        self.values.append(value)

    def clear(self):
        self.__init__()

    def compute_returns_and_advantages(self, last_value, gamma=GAMMA, lam=LAM):
        # convert to numpy arrays
        rewards = np.array(self.rewards, dtype=np.float32)
        values = np.array(self.values + [last_value], dtype=np.float32)
        dones = np.array(self.dones, dtype=np.float32)
        deltas = rewards + gamma * values[1:] * (1 - dones) - values[:-1] 
        advantages = np.zeros_like(deltas)
        adv = 0.0
        for t in reversed(range(len(deltas))):
            adv = deltas[t] + gamma * lam * (1 - dones[t]) * adv
            advantages[t] = adv
        returns = advantages + values[:-1]
        # convert to tensors
        obs = torch.tensor(np.array(self.obs), dtype=torch.float32, device=DEVICE)
        actions = torch.tensor(self.actions, dtype=torch.long, device=DEVICE)
        logprobs = torch.stack(self.logprobs).to(DEVICE).detach()
        values = torch.tensor(self.values, dtype=torch.float32, device=DEVICE)
        advantages = torch.tensor(advantages, dtype=torch.float32, device=DEVICE)
        returns = torch.tensor(returns, dtype=torch.float32, device=DEVICE)
        # normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        return obs, actions, logprobs, values, returns, advantages

# --- PPO training function ---
def ppo_train():
    env = gym.make(ENV_NAME)
    # env.seed(SEED)
    obs_dim = env.observation_space.shape[0]
    act_dim = env.action_space.n

    print(f"Observation space: {obs_dim}, Action space: {act_dim}")

    model = ActorCritic(obs_dim, act_dim).to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=LR)

    buffer = RolloutBuffer()
    episode_rewards = deque(maxlen=100)

    obs = env.reset()[0]
    ep_reward = 0
    total_steps = 0

    for update in range(1, TOTAL_UPDATES + 1):
        # collect rollouts
        # print(f"=== Update {update} ===")
        for step in range(STEPS_PER_UPDATE):
            # print("obs:", obs)
            obs_tensor = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)
            # print("step:", step)
            # print(obs_tensor)
            # print(obs_tensor.shape)
            with torch.no_grad():
                logits, value = model.forward(obs_tensor)
                # print("logits:", logits)
                # print("value:", value)
                dist = Categorical(logits=logits)
                # print("dist:", dist)
                action = dist.sample().cpu().numpy()[0]
                # print("action:", action)
                logp = dist.log_prob(torch.tensor(action, device=DEVICE))
                # print("logp:", logp)
            next_obs, reward, done, info, _ = env.step(int(action))
            # print("next_obs:", next_obs)
            # print("reward:", reward)
            # print("done:", done)
            # print("info:", info)
            buffer.add(obs, action, reward, done, logp, value.item())
            obs = next_obs
            ep_reward += reward
            total_steps += 1

            if done:
                # print(f"Episode finished after {step+1} steps.")
                episode_rewards.append(ep_reward)
                # print(f"Episode reward: {ep_reward}")
                obs = env.reset()[0]
                ep_reward = 0
        # print("Episode rewards (last 10):", list(episode_rewards)[-10:])
        # print(f"Collected {STEPS_PER_UPDATE} steps.")

        # compute last value for bootstrap
        obs_tensor = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)
        with torch.no_grad():
            _, last_value = model.forward(obs_tensor)
            last_value = last_value.item()

        # prepare training data
        obs_b, actions_b, old_logprobs_b, values_b, returns_b, adv_b = buffer.compute_returns_and_advantages(last_value)
        buffer.clear()

        # PPO update: multiple epochs, minibatches
        dataset_size = obs_b.size(0)
        for epoch in range(UPDATE_EPOCHS):
            # generate permutation for minibatches
            perm = torch.randperm(dataset_size)
            for start in range(0, dataset_size, MINI_BATCH_SIZE):
                idx = perm[start:start + MINI_BATCH_SIZE]
                mb_obs = obs_b[idx]
                mb_actions = actions_b[idx]
                mb_old_logprobs = old_logprobs_b[idx]
                mb_returns = returns_b[idx]
                mb_adv = adv_b[idx]

                # current policy
                logits, values = model.forward(mb_obs)
                dist = Categorical(logits=logits)
                mb_logprobs = dist.log_prob(mb_actions)
                entropy = dist.entropy().mean()

                # ratio for clipping
                ratio = torch.exp(mb_logprobs - mb_old_logprobs)

                # clipped surrogate objective
                surrogate1 = ratio * mb_adv #
                surrogate2 = torch.clamp(ratio, 1.0 - CLIP_EPS, 1.0 + CLIP_EPS) * mb_adv
                policy_loss = -torch.min(surrogate1, surrogate2).mean()

                # value loss (MSE)
                value_loss = (mb_returns - values).pow(2).mean()

                # total loss
                loss = policy_loss + VALUE_COEF * value_loss - ENTROPY_COEF * entropy

                optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)
                optimizer.step()

        # Logging
        if len(episode_rewards) > 0:
            avg_reward = sum(episode_rewards) / len(episode_rewards)
        else:
            avg_reward = 0.0
        print(f"Update {update:4d}  Steps {total_steps:7d}  AvgReward(100) {avg_reward:6.2f}")

        # (optional) early stop if environment solved
        if avg_reward >= 475.0 and len(episode_rewards) >= 100:
            print("Environment solved!")
            break

    env.close()
    return model

if __name__ == "__main__":
    start_time = time.time()
    trained_model = ppo_train()
    print("Training done in {:.2f} sec".format(time.time() - start_time))


# ‡∏Ñ‡πà‡∏≤ reward ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏´‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ
# ‡∏Ñ‡πà‡∏≤ value ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡∏ô‡∏±‡πâ‡∏ô (action) ‡∏ì state ‡∏ô‡∏±‡πâ‡∏ô‡πÜ ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏à‡∏∞‡∏î‡∏µ‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô (‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏™‡∏∞‡∏™‡∏° (reward) ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏´‡∏≤‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏ô‡∏±‡πâ‡∏ô‡πÜ ‡∏ì state ‡∏ô‡∏±‡πâ‡∏ô‡πÜ) ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡πÑ‡∏ß‡πâ
# ‡∏Ñ‡πà‡∏≤ advanctage ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á value ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏°‡∏≤‡∏Å‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πâ‡∏≤‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏°‡∏≤‡∏Å‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏¢‡πà‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡πÑ‡∏ß‡πâ (‡∏Ñ‡πà‡∏≤ value) ‡∏Ñ‡πà‡∏≤ ‡∏•‡∏ö ‡πÅ‡∏¢‡πà‡∏Å‡∏ß‡πà‡∏≤ ‡∏Ñ‡πà‡∏≤‡∏ö‡∏ß‡∏Å ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤
# ‡∏Ñ‡πà‡∏≤ return ‡∏Ñ‡πà‡∏≤ value ‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ advanctage ‡πÅ‡∏•‡πâ‡∏ß
# ‡∏Ñ‡πà‡∏≤ logprobs ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à) action ‡∏ô‡∏±‡πâ‡∏ô‡πÜ‡πÉ‡∏ô state ‡∏ô‡∏±‡πâ‡∏ô‡πÜ ‡∏ó‡∏µ‡πà model ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
# ‡∏Ñ‡πà‡∏≤ ratio ‡∏Ñ‡πà‡∏≤‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤ ‡∏Ñ‡∏≤‡∏ß‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à) ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡∏Ç‡∏≠‡∏á model ‡πÄ‡∏Å‡πà‡∏≤ ‡∏Å‡∏±‡∏ö model ‡πÉ‡∏´‡∏°‡πà (new_model/old_model) (‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏±‡∏á‡∏õ‡∏£‡∏±‡∏ö weight ‡∏≠‡∏¢‡∏π‡πà) ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á logprobs ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏Å ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏Å‡∏ß‡πà‡∏≤ model ‡πÄ‡∏Å‡πà‡∏≤‡∏°‡∏≤‡∏Å
# ‡∏Ñ‡πà‡∏≤ surrogate1 = advanctage * ratio ‡πÄ‡∏£‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ ‡∏ñ‡πâ‡∏≤‡∏Ñ‡πà‡∏≤ advanctage ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏ß‡∏Å‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏î‡∏µ ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° ratio ‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ(‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤ ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö model ‡πÉ‡∏´‡πâ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÄ‡∏¢‡∏≠‡πÜ) ‡∏ñ‡πâ‡∏≤‡∏•‡∏ö ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏î‡∏µ ‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏î‡∏Ñ‡πà‡∏≤ ratio ‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ
# ‡∏Ñ‡πà‡∏≤ surrogate2 = CLIP(ratio, 1.0 - CLIP_EPS, 1.0 + CLIP_EPS) * advanctage ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡πÜ‡∏Å‡∏±‡∏ö surrogate1 ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏°‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏Å‡πá‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å
# ‡∏Ñ‡πà‡∏≤ policy loss:
### policy_loss = -torch.min(surrogate1, surrogate2).mean() ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏≠‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏ï‡∏±‡∏ß ration ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î) (‡πÉ‡∏™‡πà‡∏•‡∏ö ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å maximize ‡πÄ‡∏õ‡πá‡∏ô minimize)
# ‡∏Ñ‡πà‡∏≤ value loss:
### value_loss = (mb_returns - values).pow(2).mean() ‡∏Ñ‡πà‡∏≤ value ‡∏à‡∏£‡∏¥‡∏á‡πÜ (‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏õ‡∏à‡∏£‡∏¥‡∏á‡πÜ) - ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà model ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏ß‡πâ
# ‡∏Ñ‡πà‡∏≤ entropy ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏∏‡πà‡∏° (‡∏ó‡∏∏‡∏Å‡πÜ action ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÜ‡∏Å‡∏±‡∏ô)
# total loss
### loss = policy_loss + VALUE_COEF * value_loss - ENTROPY_COEF * entropy
### policy_loss ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏Ñ‡πà‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ï‡∏¥‡∏î‡∏•‡∏ö ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÉ‡∏î‡∏Å‡πá‡πÑ‡∏î‡πâ ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á model ‡∏Å‡πá‡∏à‡∏∞‡∏¢‡∏¥‡πà‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å ‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏´‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å‡πÜ ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤ loss ‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏•‡∏î‡∏•‡∏á)
### value_loss ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ 0 (‡∏ñ‡πâ‡∏≤‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ 0 ‡πÅ‡∏•‡∏∞‡∏à‡∏∞‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ weight ‡∏Ç‡∏≠‡∏á model ‡∏ô‡πâ‡∏≠‡∏¢‡∏°‡∏≤‡∏Å)
### entropy ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏Å‡πÜ ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action (‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏∏‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡πÄ‡∏Å‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏•‡∏≠‡∏á‡πÉ‡∏ô ‡∏´‡∏•‡∏≤‡∏¢‡πÜ action ‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö action ‡πÄ‡∏î‡∏¥‡∏°‡πÜ ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÅ‡∏£‡∏Å‡πÜ ‡∏≠‡∏¥‡∏ó‡∏ò‡∏¥‡∏û‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏•‡∏î‡∏•‡∏á‡πÑ‡∏õ‡πÄ‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠ model ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏õ‡∏ô‡∏≤‡∏ô‡πÜ‡∏Ç‡∏∂‡πâ‡∏ô) ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Agent ‡∏ï‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Local Optimum


# ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡∏∞‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏¢‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö üëç

# ---

# ### **Reward (‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•)**

# **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß:**
# **Reward** ‡∏Ñ‡∏∑‡∏≠ **‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏ï‡∏≠‡∏ö‡∏£‡∏±‡∏ö‡∏ó‡∏±‡∏ô‡∏ó‡∏µ** ‡∏ó‡∏µ‡πà Agent ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏° (Environment) ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏≥ Action ‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÜ ‡πÉ‡∏ô State ‡∏ì ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏°‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ Action ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡∏ó‡∏≥‡πÑ‡∏õ‡∏ô‡∏±‡πâ‡∏ô "‡∏î‡∏µ" ‡∏´‡∏£‡∏∑‡∏≠ "‡πÑ‡∏°‡πà‡∏î‡∏µ" ‡∏ì **‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏ô‡∏±‡πâ‡∏ô**

# * **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á CartPole:**
#     * ‡πÑ‡∏î‡πâ `+1` reward ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÜ step ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏•‡πâ‡∏°
#     * ‡∏à‡∏ö‡πÄ‡∏Å‡∏° (‡πÑ‡∏°‡πâ‡∏•‡πâ‡∏°) reward ‡πÄ‡∏õ‡πá‡∏ô `0` ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÑ‡∏õ‡∏ï‡πà‡∏≠

# Reward ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô "‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô" ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏™‡πà‡∏á‡∏ú‡∏•‡∏î‡∏µ‡∏ï‡πà‡∏≠ "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏á‡∏Ñ‡∏±‡πà‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß" ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà

# ---

# ### **Value (‡∏Ñ‡πà‡∏≤ Value)**

# **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß:**
# **Value** ‡∏´‡∏£‡∏∑‡∏≠ **State-Value Function ($V(s)$)** ‡∏Ñ‡∏∑‡∏≠ **‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏£‡∏ß‡∏°‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï (Total Future Reward)** ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏à‡∏≤‡∏Å State ‡∏ô‡∏±‡πâ‡∏ô‡πÜ ‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡∏ï‡∏≤‡∏° Policy (‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à) ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏à‡∏ô‡∏à‡∏ö‡πÄ‡∏Å‡∏°

# ‡∏û‡∏π‡∏î‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏Ñ‡∏∑‡∏≠ ‡∏°‡∏±‡∏ô‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤ "‡∏Å‡∏≤‡∏£‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô State ‡∏ô‡∏µ‡πâ ‡∏°‡∏±‡∏ô‡∏î‡∏µ‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß?"

# * **‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:** ‡∏ñ‡πâ‡∏≤ Reward ‡∏Ñ‡∏∑‡∏≠ "‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô" ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ Value ‡∏Å‡πá‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏™‡∏°‡∏∑‡∏≠‡∏ô "‡∏®‡∏±‡∏Å‡∏¢‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï" ‡∏à‡∏≤‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏á‡∏≤‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
# * **‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î:** `self.value` head ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ ‡∏°‡∏±‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà "‡∏Ñ‡∏≤‡∏î‡πÑ‡∏ß‡πâ" (Expected Value) ‡∏ß‡πà‡∏≤‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏à‡∏≤‡∏Å‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ

# ---

# ### **Advantage (‡∏Ñ‡πà‡∏≤ Advantage)**

# **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß:**
# **Advantage Function ($A(s,a)$)** ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏ß‡πà‡∏≤ Action ‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÜ ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏≥‡πÉ‡∏ô State ‡∏ô‡∏±‡πâ‡∏ô‡πÜ **‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢** ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡πÑ‡∏ß‡πâ‡∏à‡∏≤‡∏Å State ‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏≤‡∏Å‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô

# ‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏Ñ‡∏∑‡∏≠: $A(s, a) = Q(s, a) - V(s)$

# * $Q(s, a)$: ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤ Value ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏ñ‡πâ‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Action `a` ‡πÉ‡∏ô State `s`
# * $V(s)$: ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤ Value ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á State `s` (‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ó‡∏∏‡∏Å Action ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏° Policy)

# **‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏á‡πà‡∏≤‡∏¢‡πÜ:**
# * **Advantage > 0 (‡∏Ñ‡πà‡∏≤‡∏ö‡∏ß‡∏Å):** Action ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ó‡∏≥‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡πÑ‡∏ß‡πâ (‡πÄ‡∏õ‡πá‡∏ô "Surprise" ‡∏ó‡∏µ‡πà‡∏î‡∏µ) ‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥ Action ‡∏ô‡∏µ‡πâ‡∏ö‡πà‡∏≠‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
# * **Advantage < 0 (‡∏Ñ‡πà‡∏≤‡∏•‡∏ö):** Action ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ó‡∏≥‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏¢‡πà‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡πÑ‡∏ß‡πâ (‡πÄ‡∏õ‡πá‡∏ô "Surprise" ‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á) ‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥ Action ‡∏ô‡∏µ‡πâ‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á

# ‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ **Generalized Advantage Estimation (GAE)** ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Advantage ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Ç‡∏∂‡πâ‡∏ô

# ---

# ### **Return (‡∏Ñ‡πà‡∏≤ Return)**

# **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß:**
# **Return ($G_t$)** ‡∏Ñ‡∏∑‡∏≠ **‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á Reward ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏£‡∏¥‡∏á** ‡∏ô‡∏±‡∏ö‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏ß‡∏•‡∏≤ $t$ ‡πÑ‡∏õ‡∏à‡∏ô‡∏à‡∏ö Episode (‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡∏•‡∏î‡∏ó‡∏≠‡∏ô‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏î‡πâ‡∏ß‡∏¢ `gamma`) ‡∏°‡∏±‡∏ô‡∏Ñ‡∏∑‡∏≠ "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏£‡∏¥‡∏á‡πÜ" ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô

# **‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏∑‡∏≠:**
# 1.  Agent ‡πÄ‡∏•‡πà‡∏ô‡πÑ‡∏õ‡∏à‡∏ô‡∏à‡∏ö Episode ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏ö `STEPS_PER_UPDATE`
# 2.  ‡πÄ‡∏£‡∏≤‡∏°‡∏≠‡∏á‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì **Return ($G_t$)** ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Step
# 3.  ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ Return ($G_t$) ‡πÅ‡∏•‡∏∞ Value ‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏ß‡πâ ($V(s)$) ‡∏°‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì **Advantage**

# ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô **Return** ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (Target) ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏™‡∏≠‡∏ô Value Function ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Advantage ‡∏Ñ‡∏£‡∏±‡∏ö

# ‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ `returns = advantages + values[:-1]` ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á "Target" ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Value Function ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Advantage ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å GAE ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Return ‡∏î‡∏¥‡∏ö‡πÜ

# ---

# ### **Logprobs (‡∏Ñ‡πà‡∏≤ Log Probs)**

# **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß:**
# **Logprobs** ‡∏Ñ‡∏∑‡∏≠ **‡∏Ñ‡πà‡∏≤ Logarithm ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô (Probability)** ‡∏ó‡∏µ‡πà Policy ‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Action ‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÜ ‡πÉ‡∏ô State ‡∏ô‡∏±‡πâ‡∏ô‡πÜ

# * **‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Log?**
#     1.  **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏ó‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (Numerical Stability):** ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0-1 ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏π‡∏ì‡πÄ‡∏•‡∏Ç‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°‡πÄ‡∏•‡πá‡∏Å‡πÜ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ 0 ‡∏à‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (underflow) ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Log ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏π‡∏ì‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏ß‡∏Å ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤
#     2.  **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:** ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Optimization ‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏≠‡∏ô‡∏∏‡∏û‡∏±‡∏ô‡∏ò‡πå (derivative) ‡∏Ç‡∏≠‡∏á‡∏ú‡∏•‡∏ö‡∏ß‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏Ñ‡∏π‡∏ì

# ‡∏Ñ‡πà‡∏≤ Logprobs ‡∏à‡∏∂‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ó‡∏ô "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à" ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Action ‡∏ô‡∏±‡πâ‡∏ô‡πÜ ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Log scale

# ---

# ### **Ratio (‡∏Ñ‡πà‡∏≤ Ratio)**

# **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß:**
# **Ratio** ‡∏Ñ‡∏∑‡∏≠ **‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô** ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Policy **‡πÉ‡∏´‡∏°‡πà** (‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï) ‡∏Å‡∏±‡∏ö Policy **‡πÄ‡∏Å‡πà‡∏≤** (‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•) ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Action ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô

# $$\text{ratio} = \frac{\pi_{\text{new}}(a|s)}{\pi_{\text{old}}(a|s)}$$

# * **Ratio > 1:** Policy ‡πÉ‡∏´‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Action ‡∏ô‡∏µ‡πâ **‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤** Policy ‡πÄ‡∏Å‡πà‡∏≤
# * **Ratio < 1:** Policy ‡πÉ‡∏´‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Action ‡∏ô‡∏µ‡πâ **‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤** Policy ‡πÄ‡∏Å‡πà‡∏≤
# * **‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î:** ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å `torch.exp(mb_logprobs - mb_old_logprobs)` ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ $e^{(\log(a) - \log(b))} = e^{\log(a/b)} = a/b$

# Ratio ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á PPO ‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡πÄ‡∏£‡∏≤‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Policy ‡πÑ‡∏õ‡∏°‡∏≤‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô

# ---

# ### **Surrogate Objectives ‡πÅ‡∏•‡∏∞ Policy Loss**

# **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß:**
# ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö Policy ‡πÑ‡∏õ‡πÉ‡∏ô‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ Advantage ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏ß‡∏Å

# * `surrogate1 = ratio * mb_adv`: ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô ‡∏ñ‡πâ‡∏≤ `mb_adv` ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏ß‡∏Å (‡∏î‡∏µ) ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏° `ratio` (‡∏ó‡∏≥ Action ‡∏ô‡∏µ‡πâ‡∏ö‡πà‡∏≠‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô) ‡∏ñ‡πâ‡∏≤ `mb_adv` ‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏ö (‡πÅ‡∏¢‡πà) ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏≠‡∏¢‡∏≤‡∏Å‡∏•‡∏î `ratio` (‡∏ó‡∏≥ Action ‡∏ô‡∏µ‡πâ‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á) **‡πÅ‡∏ï‡πà...** ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö `ratio` ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏û‡∏±‡∏á‡πÑ‡∏î‡πâ!

# * `surrogate2 = torch.clamp(ratio, 1.0 - CLIP_EPS, 1.0 + CLIP_EPS) * mb_adv`: ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô "‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢" ‡∏Ç‡∏≠‡∏á‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ‡∏°‡∏±‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£ **Clip** ‡∏´‡∏£‡∏∑‡∏≠ **‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á `ratio`** ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏õ‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏¥‡∏° (‡∏Ñ‡∏∑‡∏≠ 1.0) ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤ `CLIP_EPS` (‡πÄ‡∏ä‡πà‡∏ô 0.2) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ó‡∏µ‡πà‡∏Å‡πâ‡∏≤‡∏ß‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ

# * `policy_loss = -torch.min(surrogate1, surrogate2).mean()`: PPO ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà **‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤** ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á `surrogate1` ‡πÅ‡∏•‡∏∞ `surrogate2` ‡πÄ‡∏™‡∏°‡∏≠ ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£ "‡∏°‡∏≠‡∏á‡πÇ‡∏•‡∏Å‡πÉ‡∏ô‡πÅ‡∏á‡πà‡∏£‡πâ‡∏≤‡∏¢" (Pessimistic) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏°‡∏±‡∏î‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏™‡πà‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ **‡∏•‡∏ö (`-`)** ‡∏Å‡πá‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ Maximization (‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î) ‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô Minimization (‡∏ó‡∏≥‡πÉ‡∏´‡πâ Loss ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î) ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà Optimizer ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏≥



# ---

# ### **Value Loss**

# **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß:**
# `value_loss = (mb_returns - values).pow(2).mean()`: ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏ï‡∏£‡∏á‡πÑ‡∏õ‡∏ï‡∏£‡∏á‡∏°‡∏≤ ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ß‡πà‡∏≤ Value ‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (`values`) ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢" ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏ß‡πâ (`mb_returns`) ‡∏°‡∏≤‡∏Å‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô (‡∏ß‡∏±‡∏î‡∏î‡πâ‡∏ß‡∏¢ Mean Squared Error) ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏≥‡πÉ‡∏´‡πâ Value Loss ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ 0 ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏ã‡∏∂‡πà‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤ "‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡πå" (Critic) ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ

# ---

# ### **Entropy (‡∏Ñ‡πà‡∏≤ Entropy)**

# **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß:**
# **Entropy** ‡∏Ñ‡∏∑‡∏≠ **‡∏Ñ‡πà‡∏≤‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô** ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° "‡∏™‡∏∏‡πà‡∏°" ‡∏Ç‡∏≠‡∏á Policy
# * **Entropy ‡∏™‡∏π‡∏á:** Policy ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏±‡∏á‡πÄ‡∏• ‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÉ‡∏´‡πâ‡∏´‡∏•‡∏≤‡∏¢‡πÜ Action (‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡πÜ ‡∏Å‡∏±‡∏ô) ‡∏ã‡∏∂‡πà‡∏á‡∏î‡∏µ‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£ **‡∏™‡∏≥‡∏£‡∏ß‡∏à (Exploration)** ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÅ‡∏£‡∏Å‡πÜ
# * **Entropy ‡∏ï‡πà‡∏≥:** Policy ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏™‡∏π‡∏á ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∞‡πÄ‡∏ó‡πÑ‡∏õ‡∏ó‡∏µ‡πà Action ‡πÉ‡∏î Action ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (‡∏î‡∏µ‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£ **‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå (Exploitation)** ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏õ‡∏™‡∏±‡∏Å‡∏û‡∏±‡∏Å‡πÅ‡∏•‡πâ‡∏ß)

# ---

# ### **Total Loss (Loss ‡∏£‡∏ß‡∏°)**

# **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß:**
# `loss = policy_loss + VALUE_COEF * value_loss - ENTROPY_COEF * entropy`
# Loss ‡∏£‡∏ß‡∏°‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏±‡πâ‡∏á 3 ‡∏™‡πà‡∏ß‡∏ô‡∏°‡∏≤‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÇ‡∏°‡πÄ‡∏î‡∏•:

# 1.  **`policy_loss` (‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å):** ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Policy ‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏°‡∏±‡∏î‡∏£‡∏∞‡∏ß‡∏±‡∏á
# 2.  **`VALUE_COEF * value_loss` (‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏£‡∏≠‡∏á):** ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡πà‡∏≤ (Critic)
# 3.  **`- ENTROPY_COEF * entropy` (‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢):** ‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏ß‡∏à (Exploration) ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£ "‡∏•‡∏á‡πÇ‡∏ó‡∏©" Policy ‡∏ó‡∏µ‡πà‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡πÄ‡∏£‡πá‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (Entropy ‡∏ï‡πà‡∏≥) ‡πÄ‡∏£‡∏≤‡πÉ‡∏™‡πà‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏•‡∏ö ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ **Maximize** Entropy ‡πÅ‡∏ï‡πà Optimizer ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà **Minimize** Loss ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡∏≤‡∏£ Minimize "‡∏•‡∏ö Entropy" ‡∏à‡∏∂‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Maximize Entropy ‡∏ô‡∏±‡πà‡∏ô‡πÄ‡∏≠‡∏á

# `VALUE_COEF` ‡πÅ‡∏•‡∏∞ `ENTROPY_COEF` ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô