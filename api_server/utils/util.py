import io
import base64
import uuid
import re
import os
import requests
import dotenv
import psycopg2
import fitz  # PyMuPDF
import time
import mimetypes # For guessing mime types

from PIL import Image
from sentence_transformers import SentenceTransformer
from transformers import BitsAndBytesConfig, AutoModel
import torch

# New imports for Docling and concurrent processing
from docling_core.types.doc import ImageRefMode, PictureItem, TableItem
from docling.datamodel.base_models import InputFormat
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import DocumentStream
from docling_core.types.doc import DocItemLabel
import concurrent.futures
from docling.pipeline.vlm_pipeline import VlmPipeline
from docling.datamodel.pipeline_options import (
    VlmPipelineOptions,
    PdfPipelineOptions,
    TesseractCliOcrOptions,
    TesseractOcrOptions,
    RapidOcrOptions,
)
from modelscope import snapshot_download
from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions
from docling.datamodel import vlm_model_specs
from docling.datamodel.pipeline_options_vlm_model import ApiVlmOptions, ResponseFormat
from docling.pipeline.vlm_pipeline import VlmPipeline

import base64
import io
from unstructured.partition.auto import partition
from unstructured.documents.elements import Image as UnstructuredImage

# --- NEW IMPORTS ---
from minio import Minio
from minio.error import S3Error
from typing import List, Optional, Dict, Any, Union

dotenv.load_dotenv()

# Function to calculate parameter memory
def get_model_memory(model):
    total_params = sum(p.numel() for p in model.parameters())  # Total number of parameters
    total_memory_bytes = sum(p.numel() * p.element_size() for p in model.parameters())  # In bytes
    total_memory_mb = total_memory_bytes / (1024 ** 2)  # Convert to MB
    total_memory_gb = total_memory_mb / 1024  # Convert to GB
    
    print(f"Total Parameters: {total_params:,}")
    print(f"Total Memory: {total_memory_gb:.2f} GB ({total_memory_mb:.2f} MB)")
    return total_memory_gb


# ==============================================================================
#  DATABASE & MINIO CLIENT SETUP
# ==============================================================================

# This model remains for text embedding (Legacy Mode), unchanged.
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2').to(device='cuda')
# model = SentenceTransformer("jinaai/jina-embeddings-v4", trust_remote_code=True, device = "cpu").half()

# Optional: Configure 8-bit quantization (default settings work for most cases)
# quantization_config = BitsAndBytesConfig(
#     load_in_8bit=True,
#     llm_int8_threshold=6.0,  # Adjust if needed for your use case
#     llm_int8_has_fp16_weights=True,
#     llm_int8_enable_fp32_cpu_offload=True,
# )

# Step 1: Configure INT4 quantization (signed 4-bit integers)
# quantization_config = BitsAndBytesConfig(
#     load_in_4bit=True,  # Enable 4-bit
#     bnb_4bit_quant_type="nf4",  # Specifically INT4 (signed integers; use "nf4" if you want normalized floats instead)
#     bnb_4bit_compute_dtype=torch.float16,  # Dequantize/compute in FP16 for better speed and accuracy
#     bnb_4bit_use_double_quant=True,  # Optional: Nested quantization for ~0.4 extra bits savings
#     bnb_4bit_quant_storage=torch.uint8  # Internal storage format (doesn't affect output precision)
# )

# base_model = AutoModel.from_pretrained(
#     "jinaai/jina-embeddings-v4",
#     trust_remote_code=True,
#     quantization_config=quantization_config,
#     device_map="cpu",  # Automatically maps to GPU (or CPU if needed)
#     torch_dtype=torch.float16,  # Combine with FP16 for better perf
# )

# model = SentenceTransformer(modules=[base_model], device = "cpu")

uses_mem = get_model_memory(model)

# Now move to GPU
# model.to("cuda")

# --- NEW: MinIO Client Initialization ---
minio_client = Minio(
    os.getenv("MINIO_ENDPOINT", "127.0.0.1:9010") + ":" + os.getenv("MINIO_PORT", "9010"),
    access_key=os.getenv("MINIO_ACCESS_KEY", "minioadmin"),
    secret_key=os.getenv("MINIO_SECRET_KEY", "minioadmin"),
    secure=os.getenv("MINIO_USE_SSL", "false").lower() == 'true'
)
minio_bucket_name = os.getenv("MINIO_BUCKET", "user-files")

# --- NEW: Database Connection Helper ---
def get_db_connection():
    """Helper function to get a new database connection."""
    try:
        conn = psycopg2.connect(
            dbname=os.getenv("PGDATABASE", "ai_agent"),
            user=os.getenv("PGUSER", "athip"),
            password=os.getenv("PGPASSWORD", "123456"),
            host=os.getenv("PGHOST", "localhost"),
            port=os.getenv("PGPORT", "5432"),
        )
        return conn
    except psycopg2.DatabaseError as e:
        print(f"‚ùå Failed to connect to database: {e}")
        return None

# ==============================================================================
#  NEW: MINIO & FILE PROCESSING HELPERS
# ==============================================================================

def upload_file_to_minio_and_db(user_id: int, chat_history_id: int, file_name: str, file_bytes: bytes, mime_type: str = None) -> tuple[Optional[int], Optional[str]]:
    """
    Uploads a file to MinIO and creates a record in the 'uploaded_files' table.
    Mirrors the logic from database.js.

    Returns:
        (uploaded_file_id, object_name) or (None, None) on failure.
    """
    if not mime_type:
        mime_type, _ = mimetypes.guess_type(file_name)
        if not mime_type:
            mime_type = 'application/octet-stream' # Default
            
    file_size = len(file_bytes)
    object_name = f"user_{user_id}/chat_{chat_history_id}/{int(time.time())}-{file_name}"
    
    conn = None
    try:
        # 1. Upload to MinIO
        file_stream = io.BytesIO(file_bytes)
        minio_client.put_object(
            minio_bucket_name,
            object_name,
            file_stream,
            file_size,
            content_type=mime_type
        )
        print(f"‚úÖ MinIO: File '{object_name}' uploaded successfully.")

        # 2. Insert record into PostgreSQL
        conn = get_db_connection()
        if not conn:
            raise Exception("Could not connect to database")
            
        cur = conn.cursor()
        query = """
            INSERT INTO uploaded_files (user_id, chat_history_id, file_name, object_name, mime_type, file_size_bytes)
            VALUES (%s, %s, %s, %s, %s, %s)
            RETURNING id;
        """
        values = (user_id, chat_history_id, file_name, object_name, mime_type, file_size)
        cur.execute(query, values)
        uploaded_file_id = cur.fetchone()[0]
        
        conn.commit()
        cur.close()
        
        print(f"‚úÖ DB: Saved file record with ID: {uploaded_file_id}")
        return uploaded_file_id, object_name

    except Exception as e:
        print(f"‚ùå Error during file upload process: {e}")
        # Attempt to clean up MinIO object if DB insert fails or MinIO fails
        try:
            minio_client.remove_object(minio_bucket_name, object_name)
            print(f"üßπ Cleaned up MinIO object '{object_name}' after error.")
        except S3Error as cleanup_error:
            print(f"‚ö†Ô∏è Failed to clean up MinIO object '{object_name}': {cleanup_error}")
        if conn:
            conn.rollback()
        return None, None
    finally:
        if conn:
            conn.close()


def get_file_from_minio(object_name: str) -> Optional[bytes]:
    """Retrieves a file's content from MinIO as bytes."""
    try:
        response = minio_client.get_object(minio_bucket_name, object_name)
        file_bytes = response.read()
        return file_bytes
    except S3Error as e:
        print(f"‚ùå Error getting file '{object_name}' from MinIO: {e}")
        return None
    finally:
        if 'response' in locals() and response:
            response.close()
            response.release_conn()

def convert_pdf_page_to_image(pdf_bytes: bytes, page_number_0_indexed: int, dpi: int = 100) -> Optional[bytes]:
    """
    Extracts a single page from a PDF as a high-quality PNG image.
    
    Args:
        pdf_bytes: The byte content of the entire PDF.
        page_number_0_indexed: The page to extract (0 for page 1, 1 for page 2, etc.)
        dpi: The resolution in dots per inch for the output image.
    """
    try:
        pdf_document = fitz.open(stream=pdf_bytes, filetype="pdf")
        if page_number_0_indexed >= len(pdf_document):
            print(f"Error: Page {page_number_0_indexed} out of bounds.")
            pdf_document.close()
            return None
            
        page = pdf_document.load_page(page_number_0_indexed)
        
        # Render page to a pixmap (image)
        # Use high DPI (e.g., 200) for good quality
        pix = page.get_pixmap(dpi=dpi) 
        
        # Convert pixmap to PNG bytes
        img_bytes = pix.tobytes("png")
        pdf_document.close()
        return img_bytes
        
    except Exception as e:
        print(f"‚ùå Error converting PDF page {page_number_0_indexed} to image: {e}")
        return None


# ==============================================================================
#  LEGACY: DOCLING / UNSTRUCTURED (No changes from your code)
# ==============================================================================

def extract_images_from_pdf(pdf_bytes: bytes) -> list:
    """
    Extracts all images from PDF bytes using PyMuPDF and returns them as a list of bytes.
    """
    images = []
    try:
        pdf_document = fitz.open(stream=pdf_bytes, filetype="pdf")
        print(f"üìÑ PDF has {pdf_document.page_count} pages. Extracting images...")
        for page_number in range(len(pdf_document)):
            page = pdf_document.load_page(page_number)
            image_list = page.get_images(full=True)
            for img_index, img in enumerate(image_list):
                xref = img[0]
                base_image = pdf_document.extract_image(xref)
                image_bytes = base_image["image"]
                images.append(image_bytes)
        print(f"üñºÔ∏è Successfully extracted {len(images)} images from the PDF.")
    except Exception as e:
        print(f"‚ùå Error extracting images with PyMuPDF: {e}")
    return images


def extract_images_from_pdf_with_docling(pdf_bytes: bytes) -> list:
    """
    Extracts images from a PDF using the Docling pipeline.
    This method is more accurate as it identifies semantic PictureItems
    instead of just raw image objects like PyMuPDF.
    """
    images = []
    doc_stream = DocumentStream(name="temp_doc.pdf", stream=io.BytesIO(pdf_bytes))
    
    print("üß† Using Docling model to identify and extract images...")
    
    try:
        # --- Configure a standard OCR pipeline for layout analysis ---
        # This setup is efficient for finding image blocks without running a full VLM.
        # print("Downloading RapidOCR models for layout analysis...")
        # download_path = snapshot_download(repo_id="RapidAI/RapidOCR")
        # det_model_path = os.path.join(download_path, "onnx", "PP-OCRv5", "det", "ch_PP-OCRv5_server_det.onnx")
        # rec_model_path = os.path.join(download_path, "onnx", "PP-OCRv5", "rec", "ch_PP-OCRv5_rec_server_infer.onnx")
        # cls_model_path = os.path.join(download_path, "onnx", "PP-OCRv4", "cls", "ch_ppocr_mobile_v2.0_cls_infer.onnx")
        
        # ocr_options = RapidOcrOptions(
        #     det_model_path=det_model_path, rec_model_path=rec_model_path, cls_model_path=cls_model_path
        # )

        # pipeline_options = PdfPipelineOptions()
        # pipeline_options.do_ocr = True # OCR helps in distinguishing text from pure image areas
        # pipeline_options.ocr_options = ocr_options


        pipeline_options = VlmPipelineOptions(
        vlm_options=vlm_model_specs.SMOLDOCLING_TRANSFORMERS,  # <-- change the model here
            # accelerator_options = AcceleratorOptions(
            #     num_threads=8, device=AcceleratorDevice.CPU
            # )
        )
        
        converter = DocumentConverter(
            format_options={InputFormat.PDF: PdfFormatOption(pipeline_cls=VlmPipeline, pipeline_options=pipeline_options)}
        )

        # converter = DocumentConverter()
        
        # --- Convert the document and iterate through items ---
        result = converter.convert(doc_stream)
        doc = result.document

        for item, _level in doc.iterate_items():
            # Check if the item is a PictureItem
            if isinstance(item, PictureItem):
                # Get the image resource (as a PIL Image)
                image_resource = item.get_image(doc)
                
                # Convert the PIL Image to bytes
                buf = io.BytesIO()
                image_resource.save(buf, format="PNG") # Save as PNG for consistency
                image_bytes = buf.getvalue()
                images.append(image_bytes)
        
        print(f"üñºÔ∏è Successfully extracted {len(images)} images using the Docling model.")
        
    except Exception as e:
        print(f"‚ùå Error extracting images with Docling: {e}")
        
    return images


def extract_images_from_doc_with_unstructured(doc_bytes: bytes, filename: str) -> list:
    """
    Extracts images from a document using the 'unstructured' library.
    """
    images = []
    print(f"üß† Using 'unstructured' library with hi_res strategy to extract images from '{filename}'...")

    try:
        # üõ†Ô∏è Configure the partitioner specifically for image extraction
        elements = partition(
            file=io.BytesIO(doc_bytes),
            file_filename=filename,
            # extract_images_in_pdf=True, 
            extract_image_block_types=['Image'],
            extract_image_block_to_payload=True,
            # extract_image_block_output_dir="./temp_images",
            infer_table_structure=True,
            languages=['tha', 'eng'],
            strategy="hi_res",
            hi_res_model_name="yolox"
        )

        # Iterate through the elements found in the document
        for element in elements:
            # Check if the element is an image and has the embedded base64 data
            if isinstance(element, UnstructuredImage) and element.metadata.image_base64:
                # Decode the base64 string back into raw image bytes
                image_bytes = base64.b64decode(element.metadata.image_base64)
                images.append(image_bytes)

        print(f"üñºÔ∏è Successfully extracted {len(images)} images using 'unstructured'.")

    except Exception as e:
        print(f"‚ùå Error extracting images with 'unstructured': {e}")

    return images


# ==============================================================================
#  LEGACY: VLM & CONTENT EXTRACTION (No changes from your code)
# ==============================================================================

def _create_deepinfra_vlm_options(model: str, prompt: str, api_key: str) -> ApiVlmOptions:
    """
    Helper function to create ApiVlmOptions specifically for DeepInfra's OpenAI-compatible endpoint.
    """
    if not api_key:
        raise ValueError("DeepInfra API key is required.")

    headers = {"Authorization": f"Bearer {api_key}"}
    print(f'prompt: {prompt[:1000]}')

    # Configuration for the remote VLM API call
    options = ApiVlmOptions(
        url="https://api.deepinfra.com/v1/openai/chat/completions",
        params=dict(
            model=model,
            max_tokens=4096,  # Maximum tokens for the response
        ),
        headers=headers,
        prompt=prompt,
        timeout=180,  # Increased timeout for potentially large documents
        scale=1.0,  # Scale down images to reduce payload size
        temperature=0.0,  # Lower temperature for more deterministic, factual output
        # MARKDOWN is a direct and useful output. DOCTAGS is another option for a more structured format.
        response_format=ResponseFormat.MARKDOWN,
        concurrency=10,  # Allow multiple concurrent requests
    )
    return options

def _create_openrouter_vlm_options(model: str, prompt: str, api_key: str) -> ApiVlmOptions:
    """
    Helper function to create ApiVlmOptions specifically for OpenRouter's API.
    """
    if not api_key:
        raise ValueError("OpenRouter API key is required.")

    headers = {"Authorization": f"Bearer {api_key}"}
    print(f'prompt: {prompt[:1000]}')

    # Configuration for the remote VLM API call
    options = ApiVlmOptions(
        url="https://openrouter.ai/api/v1/chat/completions",
        params=dict(
            model=model,
            max_tokens=4096*10,  # Maximum tokens for the response
        ),
        headers=headers,
        prompt=prompt,
        timeout=180,  # Increased timeout for potentially large documents
        scale=2.0,  # Scale down images to reduce payload size
        temperature=0.0,  # Lower temperature for more deterministic, factual output
        # MARKDOWN is a direct and useful output. DOCTAGS is another option for a more structured format.
        response_format=ResponseFormat.MARKDOWN,
        concurrency=15,  # Allow multiple concurrent requests
    )
    return options


def generate_vlm_pipeline_options(mode: str = 'local', **kwargs) -> VlmPipelineOptions:
    """
    Generates pipeline_options for the Docling VLM pipeline based on the specified mode.
    """
    if mode == 'local':
        model_name = kwargs.get('model_name', 'SMOLDOCLING_TRANSFORMERS')
        try:
            model_spec = getattr(vlm_model_specs, model_name)
        except AttributeError:
            raise ValueError(f"Model '{model_name}' not found in docling.datamodel.vlm_model_specs.")

        print(f"‚úÖ Configuring VLM pipeline for LOCAL mode with model: {model_name}")
        pipeline_options = VlmPipelineOptions(
            vlm_options=model_spec,
        )
        return pipeline_options

    elif mode == 'remote':
        model_name = kwargs.get('model_name', 'deepseek-ai/DeepSeek-OCR')
        prompt = kwargs.get('prompt', 'Convert this document page to clean, well-structured markdown. Extract all text, format tables, and describe any images or charts in detail.')
        api_key = kwargs.get('api_key') or os.getenv("OPENROUTER_API_KEY") #os.getenv("DEEPINFRA_API_KEY")
        # api_key = kwargs.get('api_key') or os.getenv("DEEPINFRA_API_KEY")
        if model_name.startswith('deepseek-ai/DeepSeek-OCR'):
            prompt = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
        elif model_name.startswith('allenai/olmOCR-2-7B-1025'):
            prompt = ""

        if not api_key:
            raise ValueError("OPENROUTER_API_KEY not found. Please provide it as a kwarg or set the environment variable.")

        print(f"‚úÖ Configuring VLM pipeline for REMOTE mode with OpenRouter model: {model_name}")
        api_vlm_options = _create_openrouter_vlm_options(
            model=model_name,
            prompt=prompt,
            api_key=api_key
        )

        # print(f"‚úÖ Configuring VLM pipeline for REMOTE mode with OpenRouter model: {model_name}")
        # api_vlm_options = _create_deepinfra_vlm_options(
        #     model=model_name,
        #     prompt=prompt,
        #     api_key=api_key
        # )        

        pipeline_options = VlmPipelineOptions(
            enable_remote_services=True,  # This is required to allow API calls
            vlm_options=api_vlm_options,
            do_picture_description=True,  # Ensure images are described
            images_scale=2
        )
        return pipeline_options

    else:
        raise ValueError(f"Invalid mode '{mode}' specified. Choose 'local' or 'remote'.")
    

# def extract_and_process_content(file_storage, option: str = 'describe', pipeline_mode: str = 'ocr', with_images: bool = True) -> str:
#     """
#     Extracts content from a file, gets image descriptions, and replaces
#     image placeholders in the markdown output with those descriptions.
    
#     NOTE: This is part of the 'legacy' flow.
#     """
#     if option not in ['describe', 'summarize', 'describe_images_only']:
#         return "Error: Invalid option provided."

#     # This function expects a file_storage object, so we wrap bytes in BytesIO
#     if isinstance(file_storage, bytes):
#         file_bytes = file_storage
#         file_storage = io.BytesIO(file_bytes)
#         file_storage.filename = "temp_file.pdf" # Mock filename
#     else:
#         file_storage.seek(0)
#         file_bytes = file_storage.read()
#         file_storage.seek(0) # Reset pointer

#     # --- Step 1: Extract images first using PyMuPDF ---
#     extracted_images_bytes = []
#     if file_storage.filename.lower().endswith('.pdf') and with_images:
#         #  extracted_images_bytes = extract_images_from_pdf(file_bytes)
#          extracted_images_bytes = extract_images_from_pdf_with_docling(file_bytes)
#         #  extracted_images_bytes = extract_images_from_doc_with_unstructured(
#         #                                                                     doc_bytes=file_bytes,
#         #                                                                     filename=file_storage.filename
#         #                                                                 )
#          extracted_images_bytes = list(reversed(extracted_images_bytes))

#     # Handle the case where only image descriptions are requested
#     if option == 'describe_images_only':
#         if not extracted_images_bytes:
#             return "No images found in the document."
#         with concurrent.futures.ThreadPoolExecutor() as executor:
#             future_to_desc = {executor.submit(image_to_describe_from_base64, img_bytes): i for i, img_bytes in enumerate(extracted_images_bytes)}
#             descriptions = [future.result() for future in concurrent.futures.as_completed(future_to_desc)]
#         return "\n\n--- IMAGE DESCRIPTION ---\n".join(descriptions)

#     # --- Step 2: Run the Docling pipeline to get text ---
#     doc_stream = DocumentStream(name=file_storage.filename, stream=io.BytesIO(file_bytes))
#     print(f"üìÑ Converting document with Docling using '{pipeline_mode}' pipeline...")
    
#     try:
#         # (The setup logic for the 'converter' object remains unchanged)
#         if pipeline_mode == 'ocr':
#            # --- ORIGINAL OCR-BASED PIPELINE SETUP ---
#             # Download RapidOCR models from Hugging Face
#             print("Downloading RapidOCR models")
#             download_path = snapshot_download(repo_id="RapidAI/RapidOCR")
#             det_model_path = os.path.join(download_path, "onnx", "PP-OCRv5", "det", "ch_PP-OCRv5_server_det.onnx")
#             rec_model_path = os.path.join(download_path, "onnx", "PP-OCRv5", "rec", "ch_PP-OCRv5_rec_server_infer.onnx")
#             cls_model_path = os.path.join(download_path, "onnx", "PP-OCRv4", "cls", "ch_ppocr_mobile_v2.0_cls_infer.onnx")
            
#             ocr_options = RapidOcrOptions(
#                 det_model_path=det_model_path, rec_model_path=rec_model_path, cls_model_path=cls_model_path
#             )

#             pipeline_options = PdfPipelineOptions()
#             pipeline_options.do_ocr = True
#             pipeline_options.do_table_structure = True
#             pipeline_options.table_structure_options.do_cell_matching = True
#             pipeline_options.ocr_options = ocr_options
            
#             converter = DocumentConverter(
#                 format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}
#             )
        
#         elif pipeline_mode in ['vlm_local', 'vlm_remote']:
#             model_name = ['allenai/olmOCR-2-7B-1025', 'deepseek-ai/DeepSeek-OCR', 'Qwen/Qwen3-Coder-30B-A3B-Instruct', 'google/gemma-3-27b-it', 'Qwen/Qwen3-VL-8B-Instruct', 'Qwen/Qwen2.5-VL-32B-Instruct', 'qwen/qwen2.5-vl-32b-instruct','x-ai/grok-4-fast', 'google/gemini-2.0-flash-001'] #qwen/qwen2.5-vl-32b-instruct
#             vlm_prompt = (
# """You are an expert Document Analyst AI. Your mission is to meticulously analyze an image of a document and convert its **entire content** into a structured Markdown format. You must process the document sequentially from top to bottom, preserving the original order of all elements.

# **Your primary directive is to perform a literal, verbatim extraction. You must not summarize, interpret, rephrase, or omit any content.**

# Your response must be a single, complete Markdown document representing the source file.

# ### Your Guiding Principles

# 1.  **Absolute Sequential Order:** Process the document from its absolute top to its absolute bottom, transcribing elements in the exact order they appear. If content is arranged visually (e.g., side-by-side columns), you must transcribe the content in a logical reading order (e.g., top-to-bottom of the left column, then top-to-bottom of the right column). The order of elements in your output **must** perfectly match the original.

# 2.  **Verbatim Text Transcription:** All text elements must be transcribed **verbatim (exactly as written)** and converted into the appropriate Markdown format. This includes:

#       * Headings (e.g., `# Title`, `## Subtitle`)
#       * Paragraphs (plain text)
#       * Lists (bulleted `*` or numbered `1.`)
#       * Code blocks (using \`\`\` fences)
#       * Bold (`**text**`) and Italic (`*text*`) styling.
#       * Any and all text labels, captions, or annotations, in the exact place they appear.

# 3.  **Literal Deconstruction of Non-Textual Elements (CRITICAL):** This is **not a summary**. This is a **textual conversion** of visual information. For any visual element that cannot be represented as text (such as images, photographs, charts, **circuit diagrams, schematics,** signatures, stamps, or logos), you must:

#       * Provide a **hyper-detailed, component-by-component, literal transcription** of the element's content.
#       * **Zero Summarization:** Your transcription must be exhaustive and deconstruct the element for someone who cannot see it. You must transcribe all labels, all data points, and all connections.
#       * **For Diagrams/Schematics:** This is the most critical task. You must *trace* and *transcribe* **every single connection** between all labeled components. This is a textual representation of the visual data, not a summary of its purpose.
#           * List each component by its label (e.g., "Arduino Pin A0/14/PC0", "Resistor R1", "Dip Switch 1").
#           * Explicitly trace what each pin connects to, including any intermediate components. (e.g., "Arduino Pin 0/PD0 connects to one side of the first switch in 'Dip Switch 1' AND to one end of a '10K-ohm' resistor. The other end of this resistor connects to '+5V'. The other side of the first switch connects to 'GND'.").
#       * **For Charts/Graphs:** State the chart type (bar, line, pie). Transcribe all axes labels, the title, and the legend. Then, list the data points or relationships shown, one by one.
#       * Enclose this entire literal deconstruction within the specific tags defined in Principle 4.

# 4.  **Tagging Format:** You must use the following tags to enclose your non-textual deconstructions. **The text *inside* the tags is the literal transcription of the visual, not a summary.**

#       * **Tables:** Enclose the entire Markdown table within `<table>` and `</table>`.
#         ```markdown
#         <table>
#         | Header 1 | Header 2 |
#         |---|---|
#         | Data 1 | Data 2 |
#         </table>
#         ```
#       * **Charts:**
#         ```markdown
#         <chart>A vertical bar chart titled 'Sales'. The X-axis is 'Month' (Jan, Feb, Mar). The Y-axis is 'Revenue'. The bar for Feb is taller than Jan.</chart>
#         ```
#       * **Images/Photos:**
#         ```markdown
#         <image>A photograph of a white coffee mug with a blue logo, sitting on a wooden desk next to a laptop.</image>
#         ```
#       * **Diagrams/Schematics (Your Most Important Tag):**
#         ```markdown
#         <diagram>
#         A hyper-detailed, connection-by-connection transcription of the schematic.
#         **Component 1 (e.g., Arduino):**
#         * Pin 1 connects to...
#         * Pin 2 connects to Resistor R1...
#         **Component 2 (e.g., 7-Segment Display):**
#         * Pin 'A' connects to the other end of Resistor R1...
#         * Pin 'Common' connects to GND...
#         (This transcription must trace all connections literally from the image.)
#         </diagram>
#         ```
#       * **Logos:**
#         ```markdown
#         <logo>A circular company logo with the text 'Innovate Corp' and a gear icon in the center.</logo>
#         ```
#       * **Signatures:**
#         ```markdown
#         <signature>A handwritten, illegible signature in blue ink.</signature>
#         ```
#       * **Stamps:**
#         ```markdown
#         <stamp>A red circular stamp with the text 'APPROVED' in the center.</stamp>
#         ```

# -----

# --- Example of Correct Approach ---

# ### Gold-Standard Example (Your Target Quality)

# The example you provided is the **perfect** model of this non-summary approach. It correctly provides a literal, connection-by-connection transcription of the visual diagram *first*, and *then* provides a verbatim transcription of all the text that follows it, in the correct order.

# **Your required output must be exactly in this format and at this level of detail:**

# ```markdown
# <diagram>
# A detailed circuit schematic showing connections between an Arduino-style board, a common cathode 7-Segment display, and an 8-position Dip Switch block labeled "‡∏î‡∏¥‡∏õ‡∏™‡∏ß‡∏¥‡∏ï‡∏ä‡πå1".

# **Connections from Arduino Board:**

# * **To 7-Segment Display (via 300-ohm resistors):**
#     * Pin A0/14/PC0 connects to a 300-ohm resistor, which connects to the 'A' segment pin.
#     * Pin A1/15/PC1 connects to a 300-ohm resistor, which connects to the 'B' segment pin.
#     * Pin A2/16/PC2 connects to a 300-ohm resistor, which connects to the 'C' segment pin.
#     * Pin A3/17/PC3 connects to a 300-ohm resistor, which connects to the 'D' segment pin.
#     * Pin A4/18/PC4 connects to a 300-ohm resistor, which connects to the 'E' segment pin.
#     * Pin A5/19/PC5 connects to a 300-ohm resistor, which connects to the 'F' segment pin.
#     * Pin 8/PB0 connects to a 300-ohm resistor, which connects to the 'G' segment pin.
# * **To Dip Switch 1 (Pull-Up Configuration):**
#     * Pin 0/PD0 connects to a node. This node is connected to one side of the first switch (leftmost) and also to one end of a 10K-ohm resistor.
#     * Pin 1/PD1 connects to a node. This node is connected to one side of the second switch and also to one end of a 10K-ohm resistor.
#     * Pin 2/PD2 connects to a node. This node is connected to one side of the third switch and also to one end of a 10K-ohm resistor.
#     * Pin 3/PD3 connects to a node. This node is connected to one side of the fourth switch and also to one end of a 10K-ohm resistor.
#     * Pin 4/PD4 connects to a node. This node is connected to one side of the fifth switch and also to one end of a 10K-ohm resistor.
#     * Pin 5/PD5 connects to a node. This node is connected to one side of the sixth switch and also to one end of a 10K-ohm resistor.
#     * Pin 6/PD6 connects to a node. This node is connected to one side of the seventh switch and also to one end of a 10K-ohm resistor.
#     * Pin 7/PD7 connects to a node. This node is connected to one side of the eighth switch (rightmost) and also to one end of a 10K-ohm resistor.

# **Component Connections:**

# * **7-Segment Display:**
#     * Segments A, B, C, D, E, F, and G are connected as described above.
#     * The common cathode pin (at the bottom) is connected to GND.
# * **Dip Switch 1 ("‡∏î‡∏¥‡∏õ‡∏™‡∏ß‡∏¥‡∏ï‡∏ä‡πå1"):**
#     * This is an 8-position switch block.
#     * One side of each of the 8 switches is connected to its respective Arduino pin (PD0-PD7) as described above.
#     * The other side of all 8 switches is connected to a common GND line.
# * **Pull-Up Resistors (10K-ohm x 8):**
#     * There are 8 10K-ohm resistors.
#     * One end of each resistor is connected to an Arduino pin node (PD0-PD7).
#     * The other end of all 8 resistors is connected to a common +5V line.

# **Power Pins:**
# * The Arduino board shows a "USB JACK".
# * A power pin block shows 3.3V, 5V, GND, GND, VIN.
# * Unconnected pins on the Arduino board include: SCL, SDA, AREF, GND, 13/PB5, 12/PB4, 11/PB3, 10/PB2, 9/PB1.
# </diagram>

# ‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà 1

# Lab 1
# Dip Switch and 7-Segment

# ‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå
# 1. Arduino Board
# 2. Digital Experiment Board
# 3. 7-Segment Board

# Checkpoint# 1: ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏à‡∏≤‡∏Å Dip-switch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏≠‡∏≠‡∏Å‡∏ó‡∏µ‡πà 7-Segment
# 1.1 ‡∏ï‡πà‡∏≠‡∏ß‡∏á‡∏à‡∏£ ‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà 1
# 1.2 ‡πÉ‡∏ä‡πâ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° Arduino IDE ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏ô Code ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á ‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Code ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡∏à‡∏≤‡∏Å Dip-Switch ‡∏ó‡∏±‡πâ‡∏á 8 ‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Switch ‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡∏ï‡∏£‡∏£‡∏Å‡∏∞ ‚ÄúHIGH‚Äù ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏≠‡∏≠‡∏Å‡∏™‡∏π‡πà 7-Segment ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
# ```
# --- End of example ---
# """)
#             vlm_mode_arg = 'local' if pipeline_mode == 'vlm_local' else 'remote'
#             pipeline_options = generate_vlm_pipeline_options(mode=vlm_mode_arg, prompt=vlm_prompt, model_name=model_name[6]) # DeepInfra 0 5 OpenRouter 6
#             converter = DocumentConverter(format_options={InputFormat.PDF: PdfFormatOption(pipeline_cls=VlmPipeline, pipeline_options=pipeline_options)})
#         else:
#             return f"Error: Invalid pipeline_mode '{pipeline_mode}'. Choose 'ocr', 'vlm_local', or 'vlm_remote'."
        
#         result = converter.convert(doc_stream)
#         doc = result.document
#     except Exception as e:
#         return f"Error processing file with Docling: {e}"

#     # Get the main markdown content, which includes the '' placeholders
#     full_content = doc.export_to_markdown()

#     # --- Step 3: Get descriptions for the images we extracted earlier ---
#     image_descriptions = []
#     if extracted_images_bytes:
#         print(f"üñºÔ∏è Found {len(extracted_images_bytes)} images. Waiting for descriptions...")
#         with concurrent.futures.ThreadPoolExecutor() as executor:
#             # Create a list of futures to maintain order
#             futures = []
#             for img_bytes in extracted_images_bytes:
#                 futures.append(executor.submit(image_to_describe_from_base64, img_bytes))
#                 time.sleep(2) # Slight delay to avoid overwhelming the API
#             for future in concurrent.futures.as_completed(futures):
#                 image_descriptions.append(future.result())
#         print("‚úÖ All image descriptions received.")

#     # --- Step 4 (MODIFIED): Replace placeholders with image descriptions ---
#     if image_descriptions:
#         print("üîÑ Replacing image placeholders with descriptions in order...")
#         for description in image_descriptions:
#             # Replace the *first* occurrence of the placeholder tag.
#             # The '1' count is crucial to ensure we replace them one by one sequentially.
#             if '' in full_content:
#                 full_content = full_content.replace(
#                     '',
#                     f"\n\n**--- IMAGE DESCRIPTION ---**\n\n{description}\n\n**--- END IMAGE ---**\n\n",
#                     1
#                 )
#             else:
#                 full_content += f"\n\n**--- IMAGE DESCRIPTION ---**\n\n{description}\n\n**--- END IMAGE ---**\n\n"

#     if option == 'summarize':
#         return summarize_text_with_llm(full_content)
#     else: # 'describe'
#         print("üìù Returning final markdown with inline image descriptions.")
#         return full_content

def open_utf8_file(file) -> str:
    """
    Opens a UTF-8 encoded text file and returns its content as a string.
    """
    try:
        with io.TextIOWrapper(file, encoding='utf-8') as f:
            content = f.read()
        return content
    except Exception as e:
        print(f"‚ùå Error reading UTF-8 file: {e}")
        return ""


# ==============================================================================
#  LEGACY: EXTRACTOR FUNCTIONS (Unchanged, they call extract_and_process_content)
# ==============================================================================

def extract_pdf_text(file_storage, option: str = 'describe', mode: str = 'vlm_remote') -> str:
    """Extracts content from a PDF. Can describe (default) or summarize."""
    return extract_and_process_content(file_storage, option, pipeline_mode=mode, with_images=False)

def extract_image_text(file_storage, option: str = 'describe') -> str:
    """Extracts content from a standalone image. Can describe (default) or summarize."""
    return image_to_describe_from_base64(file_storage.read())

def extract_txt_file(file, option: str = 'describe', mode: str = 'vlm_remote') -> str:
    """Extracts content from a .txt file. Can describe (default) or summarize."""
    return open_utf8_file(file)

def extract_docx_text(file, option: str = 'describe', mode: str = 'vlm_remote') -> str:
    """Extracts content from a .docx file. Can describe (default) or summarize."""
    return extract_and_process_content(file, option, pipeline_mode=mode, with_images=True)

def extract_pptx_text(file, option: str = 'describe', mode: str = 'vlm_remote') -> str:
    """Extracts content from a .pptx file. Can describe (default) or summarize."""
    return extract_and_process_content(file, option, pipeline_mode=mode, with_images=True)

def extract_excel_text(file, option: str = 'describe', mode: str = 'vlm_remote') -> str:
    """Extracts content from an .xlsx file. Can describe (default) or summarize."""
    return extract_and_process_content(file, option, pipeline_mode=mode, with_images=True)

def extract_xls_text(file, option: str = 'describe', mode: str = 'vlm_remote') -> str:
    """Extracts content from an .xls file. Can describe (default) or summarize."""
    return extract_and_process_content(file, option, pipeline_mode=mode, with_images=True)

def summarize_text_with_llm(text: str) -> str:
    """
    Summarizes a long text using an LLM via OpenRouter.
    """
    print("üìö Summarizing content...")
    system_prompt = "You are an expert summarizer. Your task is to provide a concise and clear summary of the given text, capturing the key points and main ideas."
    prompt = f"Please summarize the following content:\n\n---\n\n{text}\n\n---\n\nSummary:"
    
    # Using a fast and cost-effective model for summarization tasks
    summary = OpenRouterInference(prompt=prompt, system_prompt=system_prompt, model_name="x-ai/grok-4-fast")
    return summary

def image_to_describe_from_base64(image_bytes: bytes) -> str:
    """
    Extract object descriptions from an image in base64 format.
    """
    # save image bytes to a PIL Image
    image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
    # You might want to manage temporary files more carefully in production
    temp_image_path = f"./temp_images/temp_image_for_description_{uuid.uuid4()}.jpg"
    os.makedirs(os.path.dirname(temp_image_path), exist_ok=True)
    image.save(temp_image_path)
    
    # System prompt for OpenRouter VLM
    system_prompt = ("You're an image expert."
                     "If the image contains text, extract and summarize it...")
    # Prompt for OpenRouter VLM
    prompt = ("Please describe the image in detail in a text format that allows you to understand its details.")
    # Call the object detection API with the image bytes
    response = OpenRouterInference(prompt=prompt, system_prompt=system_prompt, image_bytes_list=[image_bytes], model_name="qwen/qwen2.5-vl-32b-instruct") #qwen/qwen2.5-vl-32b-instruct // qwen/qwen3-vl-8b-instruct
    return response

# ==============================================================================
#  UPDATED: VLM & EMBEDDING INFERENCE
# ==============================================================================

def OpenRouterInference(prompt: str, system_prompt: str = "", image_bytes_list: List[bytes] = None, model_name: str = "google/gemma-3-12b-it") -> str:
    """
    Perform inference using OpenRouter API with optional MULTIPLE image input for VLM.
    This version resizes images and dynamically detects the MIME type.

    Args:
        prompt: The user prompt.
        system_prompt: The system prompt to guide the model.
        image_bytes_list: Optional LIST of image bytes for VLM processing.
        model_name: The name of the OpenRouter model to use.

    Returns:
        The model's response as a string, or an error message.
    """
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        return "Error: OPENROUTER_API_KEY environment variable not set."

    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})

    # Start user content with the text prompt
    user_content = [{"type": "text", "text": prompt}]

    if image_bytes_list:
        print(f"Processing {len(image_bytes_list)} images for VLM...")
        for image_bytes in image_bytes_list:
            try:
                # Open the image from bytes
                img = Image.open(io.BytesIO(image_bytes))
                
                # --- START: RESIZING SECTION ---
                max_size = (1024, 1024)  # Max width and height of 1024 pixels
                img.thumbnail(max_size, Image.Resampling.LANCZOS)
                
                output_buffer = io.BytesIO()
                image_format = img.format or 'JPEG' 
                img.save(output_buffer, format=image_format)
                
                resized_image_bytes = output_buffer.getvalue()
                # --- END: RESIZING SECTION ---

                base64_image = base64.b64encode(resized_image_bytes).decode('utf-8')
                image_url = f"data:image/{image_format.lower()};base64,{base64_image}"
                
                # Append each image to the user_content list
                user_content.append({
                    "type": "image_url",
                    "image_url": {"url": image_url}
                })
            except Exception as e:
                print(f"Error processing, resizing, or encoding an image: {e}")
                # Optionally add a text placeholder for the failed image
                user_content.append({"type": "text", "text": "[Image processing failed]"})
    
    messages.append({"role": "user", "content": user_content})

    # --- The rest of the function remains the same ---
    try:
        response = requests.post(
            url="https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": model_name,
                "messages": messages
            }
        )
        response.raise_for_status()
        data = response.json()
        return data['choices'][0]['message']['content']

    except requests.exceptions.RequestException as e:
        return f"Error calling OpenRouter API: {e}"
    except (KeyError, IndexError) as e:
        return f"Error parsing OpenRouter response: {e}. Full response: {response.text}"
    except Exception as e:
        return f"An unexpected error occurred: {e}"



def extract_and_process_content(file_storage, option: str = 'describe', pipeline_mode: str = 'ocr', with_images: bool = True) -> str:
    """
    UPDATED: This function now processes files by converting them to images 
    and sending them to a VLM for analysis.
    
    - For PDFs: Converts each page to an image and sends all page images.
    - For Images: Sends the single image.
    - Other types (DOCX, TXT, etc.) fall back to the legacy OCR text extraction.
    """
    
    # --- Step 1: Get file bytes ---
    if isinstance(file_storage, bytes):
        file_bytes = file_storage
        file_storage = io.BytesIO(file_bytes)
        file_storage.filename = "temp_file.pdf" # Mock filename
    else:
        file_storage.seek(0)
        file_bytes = file_storage.read()
        file_storage.seek(0) # Reset pointer
        
    if not file_storage.filename:
        # Try to guess type if no filename
        file_storage.filename = "temp_file.bin"

    file_ext = os.path.splitext(file_storage.filename)[1].lower()
    image_bytes_list = []
    
    # --- Step 2: Convert file to list of images ---
    
    if file_ext == '.pdf':
        print(f"Processing PDF '{file_storage.filename}': Converting pages to images...")
        try:
            pdf_document = fitz.open(stream=file_bytes, filetype="pdf")
            page_count = len(pdf_document)
            print(f"PDF has {page_count} pages.")
            
            for page_num_0_idx in range(page_count):
                page_image_bytes = convert_pdf_page_to_image(file_bytes, page_num_0_idx, dpi=200)
                if page_image_bytes:
                    image_bytes_list.append(page_image_bytes)
                else:
                    print(f"‚ö†Ô∏è Warning: Could not convert page {page_num_0_idx + 1}")
            
            pdf_document.close()
            print(f"‚úÖ Successfully converted {len(image_bytes_list)} pages to images.")
            
        except Exception as e:
            return f"Error opening or converting PDF: {e}"
            

    elif file_ext in ['.png', '.jpg', '.jpeg', '.webp', '.bmp', '.gif']:
        print(f"Processing single image: '{file_storage.filename}'")
        image_bytes_list.append(file_bytes)
        
    else:
        # Fallback for unsupported types: Try the old OCR logic
        # This preserves functionality for DOCX, TXT, etc.
        print(f"Unsupported file type '{file_ext}' for VLM-image flow. Falling back to legacy OCR...")
        # --- ORIGINAL OCR/TEXT EXTRACTION LOGIC (Simplified) ---
        try:
            doc_stream = DocumentStream(name=file_storage.filename, stream=io.BytesIO(file_bytes))
            # print("Downloading RapidOCR models")
            # download_path = snapshot_download(repo_id="RapidAI/RapidOCR")
            # det_model_path = os.path.join(download_path, "onnx", "PP-OCRv5", "det", "ch_PP-OCRv5_server_det.onnx")
            # rec_model_path = os.path.join(download_path, "onnx", "PP-OCRv5", "rec", "ch_PP-OCRv5_rec_server_infer.onnx")
            # cls_model_path = os.path.join(download_path, "onnx", "PP-OCRv4", "cls", "ch_ppocr_mobile_v2.0_cls_infer.onnx")
            
            # ocr_options = RapidOcrOptions(
            #     det_model_path=det_model_path, rec_model_path=rec_model_path, cls_model_path=cls_model_path
            # )
            # pipeline_options = PdfPipelineOptions()
            # pipeline_options.do_ocr = True
            # pipeline_options.do_table_structure = True
            # pipeline_options.table_structure_options.do_cell_matching = True
            # pipeline_options.ocr_options = ocr_options

            # pipeline_options_word = WordPipelineOptions()
            # pipeline_options_powerpoint = PowerPointPipelineOptions()
            # pipeline_options_excel = ExcelPipelineOptions()
            
            # Use a generic converter for fallback
            converter = DocumentConverter(
                # format_options={
                #     InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options),
                #     InputFormat.DOCX: PdfFormatOption(pipeline_options=pipeline_options_word), 
                #     InputFormat.PPTX: PdfFormatOption(pipeline_options=pipeline_options_powerpoint),
                #     InputFormat.XLSX: PdfFormatOption(pipeline_options=pipeline_options_excel),
                #     # InputFormat.XLS: PdfFormatOption(pipeline_options=pipeline_options),
                #     # InputFormat.TXT: PdfFormatOption(pipeline_options=pipeline_options),
                # }
            )
            
            result = converter.convert(doc_stream)
            doc = result.document
            full_content = doc.export_to_markdown()
            
            if option == 'summarize':
                return summarize_text_with_llm(full_content)
            else:
                return full_content
                
        except Exception as e:
            return f"Error during legacy OCR fallback: {e}"
        # --- END OF FALLBACK ---

    if not image_bytes_list:
        return "Error: No images were successfully extracted or converted from the file."

    # --- Step 3: Define VLM Prompt ---
    # (This is the detailed instruction prompt from your original code)
    vlm_system_prompt = (
"""You are an expert Document Analyst AI. Your mission is to meticulously analyze an image of a document and convert its **entire content** into a structured Markdown format. You must process the document sequentially from top to bottom, preserving the original order of all elements.

**Your primary directive is to perform a literal, verbatim extraction. You must not summarize, interpret, rephrase, or omit any content.**

Your response must be a single, complete Markdown document representing the source file.

### Your Guiding Principles

1.  **Absolute Sequential Order:** Process the document from its absolute top to its absolute bottom, transcribing elements in the exact order they appear. If content is arranged visually (e.g., side-by-side columns), you must transcribe the content in a logical reading order (e.g., top-to-bottom of the left column, then top-to-bottom of the right column). The order of elements in your output **must** perfectly match the original.

2.  **Verbatim Text Transcription:** All text elements must be transcribed **verbatim (exactly as written)** and converted into the appropriate Markdown format. This includes:

      * Headings (e.g., `# Title`, `## Subtitle`)
      * Paragraphs (plain text)
      * Lists (bulleted `*` or numbered `1.`)
      * Code blocks (using \`\`\` fences)
      * Bold (`**text**`) and Italic (`*text*`) styling.
      * Any and all text labels, captions, or annotations, in the exact place they appear.

3.  **Literal Deconstruction of Non-Textual Elements (CRITICAL):** This is **not a summary**. This is a **textual conversion** of visual information. For any visual element that cannot be represented as text (such as images, photographs, charts, **circuit diagrams, schematics,** signatures, stamps, or logos), you must:

      * Provide a **hyper-detailed, component-by-component, literal transcription** of the element's content.
      * **Zero Summarization:** Your transcription must be exhaustive and deconstruct the element for someone who cannot see it. You must transcribe all labels, all data points, and all connections.
      * **For Diagrams/Schematics:** This is the most critical task. You must *trace* and *transcribe* **every single connection** between all labeled components. This is a textual representation of the visual data, not a summary of its purpose.
          * List each component by its label (e.g., "Arduino Pin A0/14/PC0", "Resistor R1", "Dip Switch 1").
          * Explicitly trace what each pin connects to, including any intermediate components. (e.g., "Arduino Pin 0/PD0 connects to one side of the first switch in 'Dip Switch 1' AND to one end of a '10K-ohm' resistor. The other end of this resistor connects to '+5V'. The other side of the first switch connects to 'GND'.").
      * **For Charts/Graphs:** State the chart type (bar, line, pie). Transcribe all axes labels, the title, and the legend. Then, list the data points or relationships shown, one by one.
      * Enclose this entire literal deconstruction within the specific tags defined in Principle 4.

4.  **Tagging Format:** You must use the following tags to enclose your non-textual deconstructions. **The text *inside* the tags is the literal transcription of the visual, not a summary.**

      * **Tables:** Enclose the entire Markdown table within `<table>` and `</table>`.
        ```markdown
        <table>
        | Header 1 | Header 2 |
        |---|---|
        | Data 1 | Data 2 |
        </table>
        ```
      * **Charts:**
        ```markdown
        <chart>A vertical bar chart titled 'Sales'. The X-axis is 'Month' (Jan, Feb, Mar). The Y-axis is 'Revenue'. The bar for Feb is taller than Jan.</chart>
        ```
      * **Images/Photos:**
        ```markdown
        <image>A photograph of a white coffee mug with a blue logo, sitting on a wooden desk next to a laptop.</image>
        ```
      * **Diagrams/Schematics (Your Most Important Tag):**
        ```markdown
        <diagram>
        A hyper-detailed, connection-by-connection transcription of the schematic.
        **Component 1 (e.g., Arduino):**
        * Pin 1 connects to...
        * Pin 2 connects to Resistor R1...
        **Component 2 (e.g., 7-Segment Display):**
        * Pin 'A' connects to the other end of Resistor R1...
        * Pin 'Common' connects to GND...
        (This transcription must trace all connections literally from the image.)
        </diagram>
        ```
      * **Logos:**
        ```markdown
        <logo>A circular company logo with the text 'Innovate Corp' and a gear icon in the center.</logo>
        ```
      * **Signatures:**
        ```markdown
        <signature>A handwritten, illegible signature in blue ink.</signature>
        ```
      * **Stamps:**
        ```markdown
        <stamp>A red circular stamp with the text 'APPROVED' in the center.</stamp>
        ```

-----

### *** Note ***
You must extract all content from the document.

### Gold-Standard Example (Your Target Quality)

The example you provided is the **perfect** model of this non-summary approach. It correctly provides a literal, connection-by-connection transcription of the visual diagram *first*, and *then* provides a verbatim transcription of all the text that follows it, in the correct order.

**Your required output must be exactly in this format and at this level of detail:**

```markdown
<diagram>
A detailed circuit schematic showing connections between an Arduino-style board, a common cathode 7-Segment display, and an 8-position Dip Switch block labeled "‡∏î‡∏¥‡∏õ‡∏™‡∏ß‡∏¥‡∏ï‡∏ä‡πå1".

**Connections from Arduino Board:**

* **To 7-Segment Display (via 300-ohm resistors):**
    * Pin A0/14/PC0 connects to a 300-ohm resistor, which connects to the 'A' segment pin.
    * Pin A1/15/PC1 connects to a 300-ohm resistor, which connects to the 'B' segment pin.
    * Pin A2/16/PC2 connects to a 300-ohm resistor, which connects to the 'C' segment pin.
    * Pin A3/17/PC3 connects to a 300-ohm resistor, which connects to the 'D' segment pin.
    * Pin A4/18/PC4 connects to a 300-ohm resistor, which connects to the 'E' segment pin.
    * Pin A5/19/PC5 connects to a 300-ohm resistor, which connects to the 'F' segment pin.
    * Pin 8/PB0 connects to a 300-ohm resistor, which connects to the 'G' segment pin.
* **To Dip Switch 1 (Pull-Up Configuration):**
    * Pin 0/PD0 connects to a node. This node is connected to one side of the first switch (leftmost) and also to one end of a 10K-ohm resistor.
    * Pin 1/PD1 connects to a node. This node is connected to one side of the second switch and also to one end of a 10K-ohm resistor.
    * Pin 2/PD2 connects to a node. This node is connected to one side of the third switch and also to one end of a 10K-ohm resistor.
    * Pin 3/PD3 connects to a node. This node is connected to one side of the fourth switch and also to one end of a 10K-ohm resistor.
    * Pin 4/PD4 connects to a node. This node is connected to one side of the fifth switch and also to one end of a 10K-ohm resistor.
    * Pin 5/PD5 connects to a node. This node is connected to one side of the sixth switch and also to one end of a 10K-ohm resistor.
    * Pin 6/PD6 connects to a node. This node is connected to one side of the seventh switch and also to one end of a 10K-ohm resistor.
    * Pin 7/PD7 connects to a node. This node is connected to one side of the eighth switch (rightmost) and also to one end of a 10K-ohm resistor.

**Component Connections:**

* **7-Segment Display:**
    * Segments A, B, C, D, E, F, and G are connected as described above.
    * The common cathode pin (at the bottom) is connected to GND.
* **Dip Switch 1 ("‡∏î‡∏¥‡∏õ‡∏™‡∏ß‡∏¥‡∏ï‡∏ä‡πå1"):**
    * This is an 8-position switch block.
    * One side of each of the 8 switches is connected to its respective Arduino pin (PD0-PD7) as described above.
    * The other side of all 8 switches is connected to a common GND line.
* **Pull-Up Resistors (10K-ohm x 8):**
    * There are 8 10K-ohm resistors.
    * One end of each resistor is connected to an Arduino pin node (PD0-PD7).
    * The other end of all 8 resistors is connected to a common +5V line.

**Power Pins:**
* The Arduino board shows a "USB JACK".
* A power pin block shows 3.3V, 5V, GND, GND, VIN.
* Unconnected pins on the Arduino board include: SCL, SDA, AREF, GND, 13/PB5, 12/PB4, 11/PB3, 10/PB2, 9/PB1.
</diagram>

‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà 1

Lab 1
Dip Switch and 7-Segment

‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå
1. Arduino Board
2. Digital Experiment Board
3. 7-Segment Board

Checkpoint# 1: ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏à‡∏≤‡∏Å Dip-switch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏≠‡∏≠‡∏Å‡∏ó‡∏µ‡πà 7-Segment
1.1 ‡∏ï‡πà‡∏≠‡∏ß‡∏á‡∏à‡∏£ ‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà 1
1.2 ‡πÉ‡∏ä‡πâ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° Arduino IDE ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏ô Code ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á ‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Code ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡∏à‡∏≤‡∏Å Dip-Switch ‡∏ó‡∏±‡πâ‡∏á 8 ‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Switch ‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡∏ï‡∏£‡∏£‡∏Å‡∏∞ ‚ÄúHIGH‚Äù ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏≠‡∏≠‡∏Å‡∏™‡∏π‡πà 7-Segment ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
```""")

    # Add context about the images
    page_references = [f"- Document Page {i+1}" for i in range(len(image_bytes_list))]
    reference_text = "\n".join(page_references)

    # This prompt is sent as the 'user' message
    # final_user_prompt = f"""

    # I am providing you with {len(image_bytes_list)} images from the file '{file_storage.filename}'. These images represent the pages of the document in sequential order: {reference_text}

    # Please analyze all these pages as a single, continuous document and generate the full Markdown extraction as requested in the system prompt. Begin processing from Page 1 and continue sequentially to the end. """

    # --- Step 4: Call OpenRouterInference ---
    print(f"Sending {len(image_bytes_list)} images to OpenRouter VLM...")

    vlm_response = ""
    batch_size = 15
    for i in range(len(image_bytes_list)//batch_size + 2): # Process in batches of 15
        print(f"Processing images {(i)*batch_size + 1} to {min((i+1)*batch_size, len(image_bytes_list))}...")
        if (i)*batch_size >= len(image_bytes_list):
            break
        vlm_response += DeepInfraInference(
            prompt=vlm_system_prompt,
            # system_prompt=vlm_system_prompt, # The user's detailed instructions go here
            image_bytes_list=image_bytes_list[(i)*batch_size:(i+1)*batch_size], # Send in batches of 5 images
            model_name="Qwen/Qwen2.5-VL-32B-Instruct" # Using a strong VLM Qwen/Qwen3-VL-8B-Instruct Qwen/Qwen3-VL-30B-A3B-Instruct Qwen/Qwen2.5-VL-32B-Instruct
        ) + "\n\n"

    print("‚úÖ VLM processing complete.")

    # --- Step 5: Handle "summarize" option ---
    if option == 'summarize':
        print("Summarizing VLM output...")
        return summarize_text_with_llm(vlm_response)
    else: # 'describe'
        return vlm_response
    
# ==============================================================================
#  DEEPINFRA FUNCTION (Updated)
# ==============================================================================

def DeepInfraInference(prompt: str, system_prompt: str = "", image_bytes_list: List[bytes] = None, model_name: str = "deepseek-ai/DeepSeek-OCR") -> str:
    """
    Perform inference using DeepInfra's OpenAI-compatible API.
    This version supports optional MULTIPLE image input for VLM, resizing,
    and returns output in the same format as OpenRouter.

    Args:
        prompt: The user prompt.
        system_prompt: The system prompt to guide the model.
        image_bytes_list: Optional LIST of image bytes for VLM processing.
        model_name: The name of the DeepInfra model to use.

    Returns:
        The model's response as a string, or an error message.
    """
    api_key = os.getenv("DEEPINFRA_API_KEY")
    if not api_key:
        return "Error: DEEPINFRA_API_KEY environment variable not set."

    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})

    # Start user content with the text prompt
    user_content = [{"type": "text", "text": prompt}]

    if image_bytes_list:
        print(f"Processing {len(image_bytes_list)} images for VLM...")
        for image_bytes in image_bytes_list:
            try:
                # Open the image from bytes
                img = Image.open(io.BytesIO(image_bytes))
                
                # --- START: RESIZING SECTION ---
                max_size = (1028, 1028)  # Max width and height of 1028 pixels
                img.thumbnail(max_size, Image.Resampling.LANCZOS)
                
                output_buffer = io.BytesIO()
                image_format = img.format or 'JPEG' 
                img.save(output_buffer, format=image_format)
                
                resized_image_bytes = output_buffer.getvalue()
                # --- END: RESIZING SECTION ---

                base64_image = base64.b64encode(resized_image_bytes).decode('utf-8')
                image_url = f"data:image/{image_format.lower()};base64,{base64_image}"
                
                # Append each image to the user_content list
                user_content.append({
                    "type": "image_url",
                    "image_url": {"url": image_url}
                })
            except Exception as e:
                print(f"Error processing, resizing, or encoding an image: {e}")
                # Optionally add a text placeholder for the failed image
                user_content.append({"type": "text", "text": "[Image processing failed]"})
    
    messages.append({"role": "user", "content": user_content})

    # --- API Call Section (Modified for DeepInfra) ---
    try:
        response = requests.post(
            # Use DeepInfra's OpenAI-compatible endpoint
            url="https://api.deepinfra.com/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": model_name,
                "messages": messages,
                'temperature': 0.0,
                'top_p': 1.0,
            }
        )
        response.raise_for_status()
        data = response.json()
        # Parse the response in the same way as OpenRouter
        return data['choices'][0]['message']['content']

    except requests.exceptions.RequestException as e:
        return f"Error calling DeepInfra API: {e}"
    except (KeyError, IndexError) as e:
        return f"Error parsing DeepInfra response: {e}. Full response: {response.text}"
    except Exception as e:
        return f"An unexpected error occurred: {e}"

def DeepInfraEmbedding(inputs: list[str], model_name: str = "Qwen/Qwen3-Embedding-0.6B") -> list[list[float]]:
    """
    Generate embeddings using the DeepInfra API.
    (Original function, unchanged)
    """
    api_key = os.getenv("DEEPINFRA_API_KEY")
    if not api_key:
        print("Error: DEEPINFRA_API_KEY environment variable not set.")
        return []

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    payload = {"inputs": inputs}
    
    api_url = f"https://api.deepinfra.com/v1/inference/{model_name}"

    try:
        response = requests.post(url=api_url, headers=headers, json=payload)
        response.raise_for_status()
        data = response.json()
        return data.get('embeddings', []) # Safely get the embeddings

    except requests.exceptions.RequestException as e:
        print(f"Error calling DeepInfra Embedding API: {e}")
        return []
    except (KeyError, IndexError) as e:
        print(f"Error parsing DeepInfra Embedding response: {e}. Full response: {response.text}")
        return []
    except Exception as e:
        print(f"An unexpected error occurred during embedding: {e}")
        return []

# --- NEW: DeepInfra CLIP Embedding Function ---
# def get_clip_embedding(text: str = None, image_bytes: bytes = None, model_name: str = "sentence-transformers/clip-ViT-B-32") -> Optional[List[float]]:
#     """
#     Gets a CLIP embedding from DeepInfra for either text or an image.
    
#     Args:
#         text: The text string to embed.
#         image_bytes: The raw image bytes to embed.
#         model_name: The CLIP model on DeepInfra.

#     Returns:
#         A 512-dimension embedding vector, or None on failure.
#     """
#     api_key = os.getenv("DEEPINFRA_API_KEY")
#     if not api_key:
#         print("Error: DEEPINFRA_API_KEY environment variable not set.")
#         return None
#     if not text and not image_bytes:
#         print("Error: Must provide either text or image_bytes to get_clip_embedding.")
#         return None

#     headers = {"Authorization": f"Bearer {api_key}"}
#     api_url = f"https://api.deepinfra.com/v1/inference/{model_name}"
    
#     payload = {}
#     if text:
#         payload = {"inputs": [text]}
#     elif image_bytes:
#         # Convert image bytes to base64 data URL
#         base64_img = base64.b64encode(image_bytes).decode('utf-8')
#         # Guess mime type, default to jpeg if unknown
#         mime_type = "image/jpeg"
#         try:
#             img = Image.open(io.BytesIO(image_bytes))
#             if img.format:
#                 mime_type = Image.MIME.get(img.format)
#         except Exception:
#             pass # Use default
        
#         data_url = f"data:{mime_type};base64,{base64_img}"
#         payload = {"image": data_url}

#     try:
#         print("Requesting CLIP embedding from DeepInfra...")
#         response = requests.post(url=api_url, headers=headers, json=payload)
#         response.raise_for_status()
#         data = response.json()
        
#         embeddings = data.get('embeddings', [])
#         if embeddings and len(embeddings) > 0:
#             print(f"‚úÖ Generated CLIP embedding using DeepInfra (Type: {'Text' if text else 'Image'}).")
#             return embeddings[0]
#         else:
#             print(f"‚ùå DeepInfra CLIP API response did not contain embeddings: {data}")
#             return None

#     except requests.exceptions.RequestException as e:
#         print(f"Error calling DeepInfra CLIP API: {e}")
#         return None
#     except Exception as e:
#         print(f"An unexpected error occurred during CLIP embedding: {e}")
#         return None


# --- UPDATED: Jina v4 Multimodal Embedding Function ---
def get_clip_embedding(
    text: str = None, 
    image_bytes_list: List[bytes] = None, 
    model_name: str = "jina-embeddings-v4"
) -> Union[Optional[List[float]], Optional[List[List[float]]]]:
    """
    Gets a multimodal embedding from Jina AI v4.

    - If 'text' is provided, embeds a single text string and returns one embedding (List[float]).
    - If 'image_bytes_list' is provided, embeds a batch of images and returns a list of embeddings (List[List[float]]).
    
    Args:
        text: The text string to embed.
        image_bytes_list: A list of raw image bytes to embed.
        model_name: The Jina model to use.

    Returns:
        A single embedding vector (List[float]) if 'text' was used.
        A list of embedding vectors (List[List[float]]) if 'image_bytes_list' was used.
        None on failure.
    """
    api_key = os.getenv("JINA_API_KEY")
    if not api_key:
        print("Error: JINA_API_KEY environment variable not set.")
        return None
    
    # Ensure only one input type is provided
    if text and image_bytes_list:
        print("Error: Provide either 'text' OR 'image_bytes_list', not both.")
        return None
    if not text and not image_bytes_list:
        print("Error: Must provide either 'text' or 'image_bytes_list' to get_clip_embedding.")
        return None

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}" # The API key from Jina dashboard
    }
    api_url = "https://api.jina.ai/v1/embeddings"
    
    input_data = []
    task_type = ""
    # create_search_prompt = f"Help me to create prompt for use to search page by text from this task or question add more detail to explain the document page. \n\n task or question : {text} \n\n Give me only new prompt no additional text."
    create_search_prompt = f"""
Act as a document search engine. 
Based on the user's query below, generate a detailed paragraph describing the content, specific keywords, and technical terminology likely to appear on a document page that answers this query. 
Do not answer the question directly; only describe the page content.

User Query: {text}

Output only the descriptive paragraph. No introductory text.
"""
    if text:
        print("Requesting Jina v4 embedding (Type: Text)...")
        search_text = DeepInfraInference(
            prompt=create_search_prompt,
            # system_prompt=system_prompt,
            # image_bytes_list=image_bytes_list,
            model_name="Qwen/Qwen3-235B-A22B-Instruct-2507" #'x-ai/grok-4-fast'#"Qwen/Qwen2.5-VL-32B-Instruct" # Use a strong VLM
        )
        print(f"Search prompt: {search_text}")
        input_data.append({"text": search_text})
        task_type = "retrieval.query"
    
    elif image_bytes_list:
        print(f"Requesting Jina v4 embedding (Type: {len(image_bytes_list)} Images)...")
        for img_bytes in image_bytes_list:
            # Jina API accepts a raw base64 string for the image
            base64_img = base64.b64encode(img_bytes).decode('utf-8')
            input_data.append({"image": base64_img})
            task_type = "retrieval.passage"

    payload = {
        "model": model_name,
        "task": task_type, # Use "text-matching" as shown in the API example for similarity
        "input": input_data
    }

    try:
        # Batch requests when sending many images to Jina to avoid large single requests / SSL errors.
        aggregated_data = []
        try:
            # If images were provided and input is large, send in batches
            if image_bytes_list and len(input_data) > 100:
                batch_size = 100
                offset = 0
                resp = None
                import random
                max_retries = 8
                for start in range(0, len(input_data), batch_size):
                    chunk = input_data[start:start + batch_size]
                    print(len(chunk))
                    chunk_payload = {
                        "model": model_name,
                        "task": task_type,
                        "input": chunk,
                        # "dimensions": 256,
                    }
                    retries = 0
                    while retries < max_retries:
                        try:
                            resp = requests.post(url=api_url, headers=headers, json=chunk_payload, timeout=120)
                            if resp.status_code == 429:
                                wait_time = (2 ** retries) + random.uniform(0, 1)
                                print(f"429 Too Many Requests. Retrying batch {start//batch_size+1} after {wait_time:.2f}s (attempt {retries+1}/{max_retries})...")
                                time.sleep(wait_time)
                                retries += 1
                                continue
                            resp.raise_for_status()
                            chunk_json = resp.json()
                            chunk_data = chunk_json.get("data", [])

                            # adjust indices so they reflect global position across batches
                            for item in chunk_data:
                                item["index"] = item.get("index", 0) + offset
                            print(len(chunk_data))
                            aggregated_data.extend(chunk_data)
                            print(len(aggregated_data))
                            offset += len(chunk)
                            print(f"batch :{((start+1)//batch_size) + 1}/{len(input_data)//batch_size} is finish")
                            time.sleep(1)
                            break
                        except requests.exceptions.RequestException as e:
                            wait_time = (2 ** retries) + random.uniform(0, 1)
                            print(f"Request error: {e}. Retrying batch {start//batch_size+1} after {wait_time:.2f}s (attempt {retries+1}/{max_retries})...")
                            time.sleep(wait_time)
                            retries += 1
                    else:
                        print(f"‚ùå Failed to get embeddings for batch {start//batch_size+1} after {max_retries} retries.")
                # emulate single combined response structure expected later
                data = {"data": aggregated_data}
                response = resp
            else:
                # single request (text or small image batch)
                max_retries = 8
                retries = 0
                import random
                while retries < max_retries:
                    response = requests.post(url=api_url, headers=headers, json=payload, timeout=120)
                    if response.status_code == 429:
                        wait_time = (2 ** retries) + random.uniform(0, 1)
                        print(f"429 Too Many Requests. Retrying after {wait_time:.2f}s (attempt {retries+1}/{max_retries})...")
                        time.sleep(wait_time)
                        retries += 1
                        continue
                    try:
                        response.raise_for_status()
                        data = response.json()
                        break
                    except requests.exceptions.RequestException as e:
                        wait_time = (2 ** retries) + random.uniform(0, 1)
                        print(f"Request error: {e}. Retrying after {wait_time:.2f}s (attempt {retries+1}/{max_retries})...")
                        time.sleep(wait_time)
                        retries += 1
                else:
                    print(f"‚ùå Failed to get embeddings after {max_retries} retries.")
                    return None
                response.raise_for_status()
                data = response.json()
        # except requests.exceptions.RequestException as e:
            # print(f"Error calling Jina Embedding API: {e}")
            # return None
        except Exception as e:
            print(f"Unexpected error during Jina embedding requests: {e}")
            return None
        
        # Parse the Jina response format
        embedding_data = data.get('data', [])
        if not embedding_data:
            print(f"‚ùå Jina API response did not contain data: {data}")
            return None

        # Sort by index to ensure order is preserved
        sorted_data = sorted(embedding_data, key=lambda x: x['index'])
        embeddings_list = [item['embedding'] for item in sorted_data]

        if not embeddings_list:
             print(f"‚ùå Jina API response did not contain valid embeddings: {data}")
             return None

        # Return a single list if text was the input (for search_similar_pages)
        if text:
            print(f"‚úÖ Generated Jina v4 embedding for text.")
            return embeddings_list[0] 
        
        # Return a list of lists if images were the input (for /process)
        elif image_bytes_list:
            print(f"‚úÖ Generated {len(embeddings_list)} Jina v4 embeddings for images.")
            return embeddings_list

    except requests.exceptions.RequestException as e:
        print(f"Error calling Jina Embedding API: {e}")
        return None
    except (KeyError, IndexError) as e:
        # This catches errors if the response structure is not as expected
        print(f"Error parsing Jina Embedding response: {e}. Full response: {response.text}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred during Jina embedding: {e}")
        return None


# def get_clip_embedding(
#     text: str = None, 
#     image_bytes_list: List[bytes] = None, 
#     model_name: str = "jina-embeddings-v4"
# ) -> Union[Optional[List[float]], Optional[List[List[float]]]]:
#     """
#     Gets a multimodal embedding from Jina AI v4.

#     - If 'text' is provided, embeds a single text string and returns one embedding (List[float]).
#     - If 'image_bytes_list' is provided, embeds a batch of images and returns a list of embeddings (List[List[float]]).
    
#     Args:
#         text: The text string to embed.
#         image_bytes_list: A list of raw image bytes to embed.
#         model_name: The Jina model to use.

#     Returns:
#         A single embedding vector (List[float]) if 'text' was used.
#         A list of embedding vectors (List[List[float]]) if 'image_bytes_list' was used.
#         None on failure.
#     """
#     api_key = os.getenv("JINA_API_KEY")
#     if not api_key:
#         print("Error: JINA_API_KEY environment variable not set.")
#         return None
    
#     # Ensure only one input type is provided
#     if text and image_bytes_list:
#         print("Error: Provide either 'text' OR 'image_bytes_list', not both.")
#         return None
#     if not text and not image_bytes_list:
#         print("Error: Must provide either 'text' or 'image_bytes_list' to get_clip_embedding.")
#         return None

#     headers = {
#         "Content-Type": "application/json",
#         "Authorization": f"Bearer {api_key}" # The API key from Jina dashboard
#     }
#     api_url = "https://api.jina.ai/v1/embeddings"
    
#     input_data = []
#     if text:
#         print("Requesting Jina v4 embedding (Type: Text)...")
#         input_data.append(text)
    
#     elif image_bytes_list:
#         print(f"Requesting Jina v4 embedding (Type: {len(image_bytes_list)} Images)...")
#         for img_bytes in image_bytes_list:
#             # Convert image bytes to base64 string before passing to the model
#             base64_img = base64.b64encode(img_bytes).decode('utf-8')
#             input_data.append(base64_img)

#     try:
#         # Batch requests when sending many images to Jina to avoid large single requests / SSL errors.
#         aggregated_data = []
#         try:
#             batch_size = 8
#             embeddings_list = model.encode(sentences=input_data, batch_size=batch_size, show_progress_bar=True, device='cpu', convert_to_numpy=True, task='retrieval', precision="int8")
       
#         except Exception as e:
#             print(f"Unexpected error during Jina embedding requests: {e}")
#             return None

#         if not embeddings_list:
#              print(f"‚ùå Jina API response did not contain valid embeddings: {None}")
#              return None

#         # Return a single list if text was the input (for search_similar_pages)
#         if text:
#             print(f"‚úÖ Generated Jina v4 embedding for text.")
#             return embeddings_list[0] 
        
#         # Return a list of lists if images were the input (for /process)
#         elif image_bytes_list:
#             print(f"‚úÖ Generated {len(embeddings_list)} Jina v4 embeddings for images.")
#             return embeddings_list

#     except requests.exceptions.RequestException as e:
#         print(f"Error calling Jina Embedding API: {e}")
#         return None
#     except (KeyError, IndexError) as e:
#         # This catches errors if the response structure is not as expected
#         print(f"Error parsing Jina Embedding response: {e}. Full response: {response.text}")
#         return None
#     except Exception as e:
#         print(f"An unexpected error occurred during Jina embedding: {e}")
#         return None
    

# ==============================================================================
#  LEGACY & NEW: DATABASE SAVE/SEARCH
# ==============================================================================
    
def encode_text_for_embedding(text: str) -> list[float]:
    """
    Convert text into an embedding vector using the DeepInfra API.
    (Original function, unchanged)
    """
    # --- NEW: Use DeepInfra for embeddings ---
    if os.getenv("DEEPINFRA_API_KEY"):
        embeddings_list = DeepInfraEmbedding(inputs=[text])
        if embeddings_list and len(embeddings_list) > 0:
            print("‚úÖ Generated embedding using DeepInfra.")
            return embeddings_list[0]
        else:
            print("‚ö†Ô∏è DeepInfra embedding failed. Falling back to local model.")
    
    # --- FALLBACK: Original SentenceTransformer logic ---
    print("Generating embedding using local SentenceTransformer model.")
    embedding = model.encode(text,device='cpu',task='retrieval')
    return embedding.tolist()

def clean_text(input_text: str) -> str:
    """
    (Original function, unchanged)
    """
    if input_text is None:
        return ""
    # Remove NUL characters
    cleaned = input_text.replace("\x00", "")
    # Optionally, remove all non-printable control chars except common whitespace (\n, \r, \t)
    cleaned = re.sub(r"[^\x20-\x7E\n\r\t]", "", cleaned)
    return cleaned

def save_vector_to_db(user_id, chat_history_id, uploaded_file_id, file_name, text, embedding, page_number: int = -1):
    """
    Save embedding to the 'document_embeddings' table (Legacy).
    
    UPDATED:
    - Takes 'uploaded_file_id' instead of 'chat_history_id' to link to the file.
    - Takes 'page_number' (defaults to -1).
    """
    # Convert Python list to PostgreSQL vector literal (e.g., '[0.1, 0.2, 0.3]')
    vector_literal = f"[{', '.join(map(str, embedding))}]"
    conn = None
    try:
        # Clean text before inserting
        text = clean_text(text)

        conn = get_db_connection()
        if not conn:
            raise Exception("Could not connect to database")
        
        cur = conn.cursor()

        query = """
            INSERT INTO document_embeddings (user_id, chat_history_id, uploaded_file_id, extracted_text, embedding, page_number)
            VALUES (%s, %s, %s, %s, %s, %s)
        """

        cur.execute(query, (user_id, chat_history_id, uploaded_file_id, text, vector_literal, page_number))

        conn.commit()
        cur.close()
        print("‚úÖ Legacy vector saved to database (page: -1).")
    except Exception as e:
        print("‚ùå Failed to save legacy vector:", e)
        if conn:
            conn.rollback()
    finally:
        if conn:
            conn.close()

# --- NEW: Save Page Vector Function ---
def save_page_vector_to_db(user_id, chat_history_id, uploaded_file_id, page_number, embedding):
    """
    Save image embedding to the 'document_page_embeddings' table (New).
    """
    vector_literal = f"[{', '.join(map(str, embedding))}]"
    conn = None
    try:
        conn = get_db_connection()
        if not conn:
            raise Exception("Could not connect to database")
            
        cur = conn.cursor()

        query = """
            INSERT INTO document_page_embeddings (user_id, chat_history_id, uploaded_file_id, page_number, embedding)
            VALUES (%s, %s, %s, %s, %s)
        """

        cur.execute(query, (user_id, chat_history_id, uploaded_file_id, page_number, vector_literal))

        conn.commit()
        cur.close()
        print(f"‚úÖ Page image vector saved to database (Page: {page_number}).")
    except Exception as e:
        print(f"‚ùå Failed to save page image vector (Page: {page_number}): {e}")
        if conn:
            conn.rollback()
    finally:
        if conn:
            conn.close()

# Search Text (Legacy)
def search_similar_documents_by_chat(query_text: str, user_id: int, chat_history_id: int, top_k: int = 5):
    """
    Search (Legacy) from 'document_embeddings' table.
    
    UPDATED:
    - Joins with 'uploaded_files' to filter by 'chat_history_id'.
    """
    # Step 1: Encode the query text to a vector
    query_embedding = encode_text_for_embedding(query_text)
    query_vector = f"[{', '.join(map(str, query_embedding))}]"
    
    conn = None
    try:
        conn = get_db_connection()
        if not conn:
            raise Exception("Could not connect to database")
            
        cur = conn.cursor()

        # Step 2: Search within same user and same chat
        # JOIN with uploaded_files to filter by chat_history_id
        query = """
            SELECT 
                t1.id, 
                t2.file_name, 
                t1.extracted_text, 
                t1.embedding <=> %s AS distance
            FROM document_embeddings AS t1
            INNER JOIN uploaded_files AS t2 ON t1.uploaded_file_id = t2.id
            WHERE t2.user_id = %s AND t2.chat_history_id = %s
            ORDER BY distance
            LIMIT %s
        """

        cur.execute(query, (query_vector, user_id, chat_history_id, top_k))
        results = cur.fetchall()

        cur.close()

        # Step 3: Return results
        return [
            {
                'id': row[0],
                'file_name': row[1],
                'text': row[2],
                'distance': row[3]
            }
            for row in results
        ]

    except Exception as e:
        print("‚ùå Failed to perform legacy similarity search:", e)
        return []
    finally:
        if conn:
            conn.close()

# --- NEW: Search Similar Pages Function ---
# Search Page (New)
from typing import List, Dict, Any
# Assuming get_clip_embedding and get_db_connection are defined elsewhere
# import { get_clip_embedding, get_db_connection } from ...

def search_similar_pages(query_text: str, user_id: int, chat_history_id: int, top_k: int = 5, threshold: float = 1.0) -> List[Dict[str, Any]]:
    """
    Search (New) from 'document_page_embeddings' table.
    
    Args:
        query_text: The user's text question.
        user_id: User's ID.
        chat_history_id: The current chat ID.
        top_k: Max number of pages to return *before* normalization/filtering.
        threshold: Max L2 distance for the *initial* SQL query. Results > threshold are excluded by SQL.
    
    Returns:
        A list of dicts, e.g.:
        [{'page_id': 12, 'file_name': 'report.pdf', ..., 'distance': 0.25, 'normalized_distance': 0.1}, ...]
    """
    # Step 1: Encode the query text using the *CLIP* model
    query_embedding = get_clip_embedding(text=query_text)
    if not query_embedding:
        print("‚ùå Failed to get CLIP embedding for query.")
        return []
        
    query_vector = f"[{', '.join(map(str, query_embedding))}]"
    
    conn = None
    try:
        conn = get_db_connection()
        if not conn:
            raise Exception("Could not connect to database")
            
        cur = conn.cursor()

        # Step 2: Search within same user and same chat
        # The SQL query remains the same, using the 'threshold' for a coarse first pass
        # and 'top_k' to limit the initial result set.
        query = """
            SELECT 
                t1.id AS page_embedding_id, 
                t2.file_name, 
                t2.object_name,
                t1.page_number,
                t1.embedding <-> %s AS distance
            FROM document_page_embeddings AS t1
            INNER JOIN uploaded_files AS t2 ON t1.uploaded_file_id = t2.id
            WHERE t2.user_id = %s 
              AND t2.chat_history_id = %s
              AND (t1.embedding <-> %s) <= %s
            ORDER BY distance
            LIMIT %s
        """

        cur.execute(query, (query_vector, user_id, chat_history_id, query_vector, threshold, top_k))
        results = cur.fetchall() # This is the raw list of tuples
        cur.close()
        print(f"‚úÖ Found {len(results)} raw pages within SQL threshold {threshold}.")

        # --- START: New logic for Normalization and 0.3 Threshold ---
        if not results:
            print("‚ÑπÔ∏è No pages found, returning empty list.")
            return []

        # Get all distances to find min/max for normalization
        all_distances = [row[4] for row in results]
        min_dist = min(all_distances)
        max_dist = max(all_distances)
        dist_range = max_dist - min_dist
        
        print(f"‚ÑπÔ∏è Normalizing distances: min={min_dist:.4f}, max={max_dist:.4f}, range={dist_range:.4f}")

        processed_results = []
        for row in results:
            original_distance = row[4]
            
            # Handle division by zero if all distances are identical (dist_range == 0)
            if dist_range == 0:
                normalized_distance = 0.0
            else:
                # Min-Max normalization: (value - min) / (max - min)
                normalized_distance = (original_distance - min_dist) / dist_range
            
            processed_results.append({
                'page_embedding_id': row[0],
                'file_name': row[1],
                'object_name': row[2],
                'page_number': row[3],
                'distance': original_distance, # The original L2 distance
                'normalized_distance': normalized_distance # The new [0,1] score
            })
        
        print(processed_results)

        # Now, filter based on the *new* normalized threshold
        final_normalized_threshold = 1.0
        final_filtered_results = [
            item for item in processed_results 
            if item['normalized_distance'] <= final_normalized_threshold
        ]
        
        print(f"‚ÑπÔ∏è Filtered to {len(final_filtered_results)} pages (normalized distance <= {final_normalized_threshold}).")
        # --- END: New logic ---

        # Step 3: Return *final filtered* results
        return final_filtered_results

    except Exception as e:
        print("‚ùå Failed to perform page similarity search:", e)
        return []
    finally:
        if conn:
            conn.close()




# def search_similar_pages(query_text: str, user_id: int, chat_history_id: int, top_k: int = 5, threshold: float = 1.0) -> List[Dict[str, Any]]:
#     """
#     Search (New) from 'document_page_embeddings' table.
    
#     Args:
#         query_text: The user's text question.
#         user_id: User's ID.
#         chat_history_id: The current chat ID.
#         top_k: Max number of pages to return *before* mean filtering.
#         threshold: Max distance (e.g., 1.0 for L2). Results with distance > threshold are excluded.
    
#     Returns:
#         A list of dicts, e.g.:
#         [{'page_id': 12, 'file_name': 'report.pdf', 'object_name': '...', 'page_number': 5, 'distance': 0.25}, ...]
#     """
#     # Step 1: Encode the query text using the *CLIP* model
#     query_embedding = get_clip_embedding(text=query_text)
#     if not query_embedding:
#         print("‚ùå Failed to get CLIP embedding for query.")
#         return []
        
#     query_vector = f"[{', '.join(map(str, query_embedding))}]"
    
#     conn = None
#     try:
#         conn = get_db_connection()
#         if not conn:
#             raise Exception("Could not connect to database")
            
#         cur = conn.cursor()

#         # Step 2: Search within same user and same chat
#         # JOIN with uploaded_files to filter by chat_history_id and get object_name
#         # Use <-> (L2 distance) operator.
#         query = """
#             SELECT 
#                 t1.id AS page_embedding_id, 
#                 t2.file_name, 
#                 t2.object_name,
#                 t1.page_number,
#                 t1.embedding <-> %s AS distance
#             FROM document_page_embeddings AS t1
#             INNER JOIN uploaded_files AS t2 ON t1.uploaded_file_id = t2.id
#             WHERE t2.user_id = %s 
#               AND t2.chat_history_id = %s
#               AND (t1.embedding <-> %s) <= %s
#             ORDER BY distance
#             LIMIT %s
#         """

#         cur.execute(query, (query_vector, user_id, chat_history_id, query_vector, threshold, top_k))
#         results = cur.fetchall() # This is the raw list of tuples
#         cur.close()
#         print(f"‚úÖ Found {len(results)} similar pages within threshold {threshold}.")
#         print("Raw Results:", results)

#         # --- START: New logic to filter by mean distance ---
#         if not results:
#             print("‚ÑπÔ∏è No pages found, returning empty list.")
#             return []

#         # Calculate mean distance from the initial results
#         all_distances = [row[4] for row in results]
#         mean_distance = sum(all_distances) / len(all_distances)
#         print(f"‚ÑπÔ∏è Calculated mean distance: {mean_distance}")

#         # Filter the results list based on the mean
#         filtered_results = [row for row in results if row[4] <= mean_distance]
#         print(f"‚ÑπÔ∏è Filtered to {len(filtered_results)} pages (distance <= mean).")
#         # --- END: New logic ---

#         # Step 3: Return *filtered* results
#         return [
#             {
#                 'page_embedding_id': row[0],
#                 'file_name': row[1],
#                 'object_name': row[2],
#                 'page_number': row[3], # 1-indexed
#                 'distance': row[4]
#             }
#             for row in filtered_results # Iterate over the new filtered list
#         ]

#     except Exception as e:
#         print("‚ùå Failed to perform page similarity search:", e)
#         return []
#     finally:
#         if conn:
#             conn.close()


# def search_similar_pages(query_text: str, user_id: int, chat_history_id: int, top_k: int = 5, threshold: float = 1.0) -> List[Dict[str, Any]]:
#     """
#     Search (New) from 'document_page_embeddings' table.
    
#     Args:
#         query_text: The user's text question.
#         user_id: User's ID.
#         chat_history_id: The current chat ID.
#         top_k: Max number of pages to return.
#         threshold: Max distance (e.g., 1.0 for L2). Results with distance > threshold are excluded.
    
#     Returns:
#         A list of dicts, e.g.:
#         [{'page_id': 12, 'file_name': 'report.pdf', 'object_name': '...', 'page_number': 5, 'distance': 0.25}, ...]
#     """
#     # Step 1: Encode the query text using the *CLIP* model
#     query_embedding = get_clip_embedding(text=query_text)
#     if not query_embedding:
#         print("‚ùå Failed to get CLIP embedding for query.")
#         return []
        
#     query_vector = f"[{', '.join(map(str, query_embedding))}]"
    
#     conn = None
#     try:
#         conn = get_db_connection()
#         if not conn:
#             raise Exception("Could not connect to database")
            
#         cur = conn.cursor()

#         # Step 2: Search within same user and same chat
#         # JOIN with uploaded_files to filter by chat_history_id and get object_name
#         # Use <-> (L2 distance) operator.
#         query = """
#             SELECT 
#                 t1.id AS page_embedding_id, 
#                 t2.file_name, 
#                 t2.object_name,
#                 t1.page_number,
#                 t1.embedding <-> %s AS distance
#             FROM document_page_embeddings AS t1
#             INNER JOIN uploaded_files AS t2 ON t1.uploaded_file_id = t2.id
#             WHERE t2.user_id = %s 
#               AND t2.chat_history_id = %s
#               AND (t1.embedding <-> %s) <= %s
#             ORDER BY distance
#             LIMIT %s
#         """

#         cur.execute(query, (query_vector, user_id, chat_history_id, query_vector, threshold, top_k))
#         results = cur.fetchall()
#         cur.close()
#         print(f"‚úÖ Found {len(results)} similar pages within threshold {threshold}.")
#         print("Results:", results)

#         # Step 3: Return results
#         return [
#             {
#                 'page_embedding_id': row[0],
#                 'file_name': row[1],
#                 'object_name': row[2],
#                 'page_number': row[3], # 1-indexed
#                 'distance': row[4]
#             }
#             for row in results
#         ]

#     except Exception as e:
#         print("‚ùå Failed to perform page similarity search:", e)
#         return []
#     finally:
#         if conn:
#             conn.close()


# --- NEW: VLM Page Processing Orchestrator ---
def process_pages_with_vlm(search_results: List[Dict[str, Any]], original_query: str) -> str:
    """
    Orchestrates the new RAG flow:
    1. Takes search results (list of pages).
    2. Fetches the original file for each from MinIO.
    3. Extracts the specific page as an image.
    4. Sends all images + query to a VLM.
    5. Returns the VLM's text response.
    """
    if not search_results:
        return "I found no relevant document pages for your query."

    print(f"üöÄ Processing {len(search_results)} relevant pages with VLM...")
    
    # Use a dictionary to cache file fetches from MinIO
    file_cache = {} 
    image_bytes_list = []
    
    page_references = [] # To tell the VLM what it's looking at

    for result in search_results:
        object_name = result['object_name']
        page_num_1_idx = result['page_number']
        file_name = result['file_name']
        
        # 1. Fetch file from cache or MinIO
        if object_name not in file_cache:
            print(f"Fetching '{object_name}' from MinIO...")
            file_bytes = get_file_from_minio(object_name)
            if file_bytes:
                file_cache[object_name] = file_bytes
            else:
                print(f"‚ö†Ô∏è Could not fetch file {object_name}. Skipping page {page_num_1_idx}.")
                continue
        
        file_bytes = file_cache[object_name]
        
        # 2. Extract the specific page as an image
        # Page number is 1-indexed in DB, convert to 0-indexed for fitz
        page_num_0_idx = page_num_1_idx - 1 
        image_bytes = convert_pdf_page_to_image(file_bytes, page_num_0_idx)
        
        if image_bytes:
            image_bytes_list.append(image_bytes)
            page_references.append(f"- '{file_name}' (Page {page_num_1_idx})")
        else:
            print(f"‚ö†Ô∏è Failed to render page {page_num_1_idx} from {file_name}. Skipping.")
            
    if not image_bytes_list:
        return "I found relevant pages but could not render them as images for analysis."

    # 3. Send all images to the VLM
    print(f"Sending {len(image_bytes_list)} images to VLM...")

    system_prompt = (
"""You are an expert Document Analyst AI. Your mission is to meticulously analyze an image of a document and convert its **entire content** into a structured Markdown format. You must process the document sequentially from top to bottom, preserving the original order of all elements.

**Your primary directive is to perform a literal, verbatim extraction. You must not summarize, interpret, rephrase, or omit any content.**

Your response must be a single, complete Markdown document representing the source file.

### Your Guiding Principles

1.  **Absolute Sequential Order:** Process the document from its absolute top to its absolute bottom, transcribing elements in the exact order they appear. If content is arranged visually (e.g., side-by-side columns), you must transcribe the content in a logical reading order (e.g., top-to-bottom of the left column, then top-to-bottom of the right column). The order of elements in your output **must** perfectly match the original.

2.  **Verbatim Text Transcription:** All text elements must be transcribed **verbatim (exactly as written)** and converted into the appropriate Markdown format. This includes:

      * Headings (e.g., `# Title`, `## Subtitle`)
      * Paragraphs (plain text)
      * Lists (bulleted `*` or numbered `1.`)
      * Code blocks (using \`\`\` fences)
      * Bold (`**text**`) and Italic (`*text*`) styling.
      * Any and all text labels, captions, or annotations, in the exact place they appear.

3.  **Literal Deconstruction of Non-Textual Elements (CRITICAL):** This is **not a summary**. This is a **textual conversion** of visual information. For any visual element that cannot be represented as text (such as images, photographs, charts, **circuit diagrams, schematics,** signatures, stamps, or logos), you must:

      * Provide a **hyper-detailed, component-by-component, literal transcription** of the element's content.
      * **Zero Summarization:** Your transcription must be exhaustive and deconstruct the element for someone who cannot see it. You must transcribe all labels, all data points, and all connections.
      * **For Diagrams/Schematics:** This is the most critical task. You must *trace* and *transcribe* **every single connection** between all labeled components. This is a textual representation of the visual data, not a summary of its purpose.
          * List each component by its label (e.g., "Arduino Pin A0/14/PC0", "Resistor R1", "Dip Switch 1").
          * Explicitly trace what each pin connects to, including any intermediate components. (e.g., "Arduino Pin 0/PD0 connects to one side of the first switch in 'Dip Switch 1' AND to one end of a '10K-ohm' resistor. The other end of this resistor connects to '+5V'. The other side of the first switch connects to 'GND'.").
      * **For Charts/Graphs:** State the chart type (bar, line, pie). Transcribe all axes labels, the title, and the legend. Then, list the data points or relationships shown, one by one.
      * Enclose this entire literal deconstruction within the specific tags defined in Principle 4.

4.  **Tagging Format:** You must use the following tags to enclose your non-textual deconstructions. **The text *inside* the tags is the literal transcription of the visual, not a summary.**

      * **Tables:** Enclose the entire Markdown table within `<table>` and `</table>`.
        ```markdown
        <table>
        | Header 1 | Header 2 |
        |---|---|
        | Data 1 | Data 2 |
        </table>
        ```
      * **Charts:**
        ```markdown
        <chart>A vertical bar chart titled 'Sales'. The X-axis is 'Month' (Jan, Feb, Mar). The Y-axis is 'Revenue'. The bar for Feb is taller than Jan.</chart>
        ```
      * **Images/Photos:**
        ```markdown
        <image>A photograph of a white coffee mug with a blue logo, sitting on a wooden desk next to a laptop.</image>
        ```
      * **Diagrams/Schematics (Your Most Important Tag):**
        ```markdown
        <diagram>
        A hyper-detailed, connection-by-connection transcription of the schematic.
        **Component 1 (e.g., Arduino):**
        * Pin 1 connects to...
        * Pin 2 connects to Resistor R1...
        **Component 2 (e.g., 7-Segment Display):**
        * Pin 'A' connects to the other end of Resistor R1...
        * Pin 'Common' connects to GND...
        (This transcription must trace all connections literally from the image.)
        </diagram>
        ```
      * **Logos:**
        ```markdown
        <logo>A circular company logo with the text 'Innovate Corp' and a gear icon in the center.</logo>
        ```
      * **Signatures:**
        ```markdown
        <signature>A handwritten, illegible signature in blue ink.</signature>
        ```
      * **Stamps:**
        ```markdown
        <stamp>A red circular stamp with the text 'APPROVED' in the center.</stamp>
        ```

-----

### *** Note ***
You must extract all content from the document.

### Gold-Standard Example (Your Target Quality)

The example you provided is the **perfect** model of this non-summary approach. It correctly provides a literal, connection-by-connection transcription of the visual diagram *first*, and *then* provides a verbatim transcription of all the text that follows it, in the correct order.

**Your required output must be exactly in this format and at this level of detail:**

```markdown
<diagram>
A detailed circuit schematic showing connections between an Arduino-style board, a common cathode 7-Segment display, and an 8-position Dip Switch block labeled "‡∏î‡∏¥‡∏õ‡∏™‡∏ß‡∏¥‡∏ï‡∏ä‡πå1".

**Connections from Arduino Board:**

* **To 7-Segment Display (via 300-ohm resistors):**
    * Pin A0/14/PC0 connects to a 300-ohm resistor, which connects to the 'A' segment pin.
    * Pin A1/15/PC1 connects to a 300-ohm resistor, which connects to the 'B' segment pin.
    * Pin A2/16/PC2 connects to a 300-ohm resistor, which connects to the 'C' segment pin.
    * Pin A3/17/PC3 connects to a 300-ohm resistor, which connects to the 'D' segment pin.
    * Pin A4/18/PC4 connects to a 300-ohm resistor, which connects to the 'E' segment pin.
    * Pin A5/19/PC5 connects to a 300-ohm resistor, which connects to the 'F' segment pin.
    * Pin 8/PB0 connects to a 300-ohm resistor, which connects to the 'G' segment pin.
* **To Dip Switch 1 (Pull-Up Configuration):**
    * Pin 0/PD0 connects to a node. This node is connected to one side of the first switch (leftmost) and also to one end of a 10K-ohm resistor.
    * Pin 1/PD1 connects to a node. This node is connected to one side of the second switch and also to one end of a 10K-ohm resistor.
    * Pin 2/PD2 connects to a node. This node is connected to one side of the third switch and also to one end of a 10K-ohm resistor.
    * Pin 3/PD3 connects to a node. This node is connected to one side of the fourth switch and also to one end of a 10K-ohm resistor.
    * Pin 4/PD4 connects to a node. This node is connected to one side of the fifth switch and also to one end of a 10K-ohm resistor.
    * Pin 5/PD5 connects to a node. This node is connected to one side of the sixth switch and also to one end of a 10K-ohm resistor.
    * Pin 6/PD6 connects to a node. This node is connected to one side of the seventh switch and also to one end of a 10K-ohm resistor.
    * Pin 7/PD7 connects to a node. This node is connected to one side of the eighth switch (rightmost) and also to one end of a 10K-ohm resistor.

**Component Connections:**

* **7-Segment Display:**
    * Segments A, B, C, D, E, F, and G are connected as described above.
    * The common cathode pin (at the bottom) is connected to GND.
* **Dip Switch 1 ("‡∏î‡∏¥‡∏õ‡∏™‡∏ß‡∏¥‡∏ï‡∏ä‡πå1"):**
    * This is an 8-position switch block.
    * One side of each of the 8 switches is connected to its respective Arduino pin (PD0-PD7) as described above.
    * The other side of all 8 switches is connected to a common GND line.
* **Pull-Up Resistors (10K-ohm x 8):**
    * There are 8 10K-ohm resistors.
    * One end of each resistor is connected to an Arduino pin node (PD0-PD7).
    * The other end of all 8 resistors is connected to a common +5V line.

**Power Pins:**
* The Arduino board shows a "USB JACK".
* A power pin block shows 3.3V, 5V, GND, GND, VIN.
* Unconnected pins on the Arduino board include: SCL, SDA, AREF, GND, 13/PB5, 12/PB4, 11/PB3, 10/PB2, 9/PB1.
</diagram>

‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà 1

Lab 1
Dip Switch and 7-Segment

‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå
1. Arduino Board
2. Digital Experiment Board
3. 7-Segment Board

Checkpoint# 1: ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏à‡∏≤‡∏Å Dip-switch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏≠‡∏≠‡∏Å‡∏ó‡∏µ‡πà 7-Segment
1.1 ‡∏ï‡πà‡∏≠‡∏ß‡∏á‡∏à‡∏£ ‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà 1
1.2 ‡πÉ‡∏ä‡πâ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° Arduino IDE ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏ô Code ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á ‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Code ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡∏à‡∏≤‡∏Å Dip-Switch ‡∏ó‡∏±‡πâ‡∏á 8 ‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Switch ‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡∏ï‡∏£‡∏£‡∏Å‡∏∞ ‚ÄúHIGH‚Äù ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏≠‡∏≠‡∏Å‡∏™‡∏π‡πà 7-Segment ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
```""")
    
    reference_text = "\n".join(page_references)
    prompt = f"""
Based on the following {len(image_bytes_list)} document pages:
{reference_text}

Please extract data to markdown (keep all original data ignore not match content) for use to answer the question:
"{original_query}"

Provide your response in Markdown format, following the tagging guidelines provided in the system prompt.
"""

    # Use OpenRouterInference, which now accepts a list of images
    # vlm_response = OpenRouterInference(
    vlm_response = DeepInfraInference(
        prompt=prompt,
        system_prompt=system_prompt,
        image_bytes_list=image_bytes_list,
        model_name="Qwen/Qwen2.5-VL-32B-Instruct" #'x-ai/grok-4-fast'#"Qwen/Qwen2.5-VL-32B-Instruct" # Use a strong VLM
    )
    print("‚úÖ VLM processing complete.")
    
    return vlm_response


# ==============================================================================
#  LEGACY: FILE SYSTEM (Unchanged)
# ==============================================================================

class EditedFileSystem:
    def __init__(self):
        # Define a base directory for file operations
        # You might want to make this configurable
        self.base_dir = "managed_files"
        if not os.path.exists(self.base_dir):
            os.makedirs(self.base_dir)

    def _get_full_path(self, file_name: str) -> str:
        """Helper to get the full path of a file within the managed directory."""
        return os.path.join(self.base_dir, file_name)

    # --- Read file operations ---
    def read_line(self, file_name: str, start_line: int, end_line: int) -> tuple[list, str]:
        """
        Reads specific lines from a file, prepending line numbers to each.
        """
        full_path = self._get_full_path(file_name)
        lines = []
        try:
            with open(full_path, 'r') as f:
                for i, line in enumerate(f, 1):
                    if start_line <= i <= end_line:
                        # clean_line = line.rstrip('\n')
                        lines.append(f"{i}: {line.rstrip()}")
                    elif i > end_line:
                        break # Optimization: stop reading once past the end_line
            return lines, ""
        except FileNotFoundError:
            return [], f"File '{file_name}' not found."
        except Exception as e:
            return [], f"An error occurred while reading file: {e}"

    def read_all(self, file_name: str) -> tuple[list, str]:
        """
        Reads all lines from a file, prepending line numbers to each.
        """
        full_path = self._get_full_path(file_name)
        lines = []
        try:
            with open(full_path, 'r') as f:
                # Enumerate to get line numbers and prepend them
                lines = [f"{i}: {line.rstrip()}" for i, line in enumerate(f, 1)]
            return lines, ""
        except FileNotFoundError:
            return [], f"File '{file_name}' not found."
        except Exception as e:
            return [], f"An error occurred while reading file: {e}"

    def read_start_until_line_n(self, file_name: str, end_line: int) -> tuple[list, str]:
        """
        Reads from the start of the file up to a specified line number,
        prepending line numbers to each.
        """
        # This method already calls read_line, which now handles line numbering
        return self.read_line(file_name, 1, end_line)

    # --- Edit file operations ---
    def edit_line(self, file_name: str, text: str, start_line: int, end_line: int) -> str:
        """
        Edits specific lines in a file by replacing them with the provided text.
        """
        full_path = self._get_full_path(file_name)
        try:
            with open(full_path, 'r') as f:
                lines = f.readlines()

            start_idx = start_line - 1
            end_idx = end_line - 1

            if not (0 <= start_idx <= end_idx < len(lines)):
                return f"Line range {start_line}-{end_line} is out of bounds for file '{file_name}'."

            new_lines_to_insert = [line + '\n' for line in text.splitlines()]
            if not new_lines_to_insert:
                new_lines_to_insert = ['\n'] # Ensure at least one newline for empty text

            new_file_content = lines[:start_idx] + new_lines_to_insert + lines[end_idx + 1:]

            with open(full_path, 'w') as f:
                f.writelines(new_file_content)
            return ""
        except FileNotFoundError:
            return f"File '{file_name}' not found for editing."
        except Exception as e:
            return f"An error occurred while editing file: {e}"

    def edit_all(self, file_name: str, text: str) -> str:
        """
        Replaces the entire content of a file with the provided text.
        """
        full_path = self._get_full_path(file_name)
        try:
            with open(full_path, 'w') as f:
                f.write(text)
            return ""
        except Exception as e:
            return f"An error occurred while editing all of file: {e}"

    # --- Create new file operations ---
    def create_new_file_only(self, file_name: str) -> str:
        """
        Creates a new empty file.
        """
        full_path = self._get_full_path(file_name)
        if os.path.exists(full_path):
            return f"File '{file_name}' already exists."
        try:
            with open(full_path, 'x') as f:
                pass
            return ""
        except FileExistsError: # Redundant due to above check, but good for robustness
            return f"File '{file_name}' already exists unexpectedly."
        except Exception as e:
            return f"An error occurred while creating new file: {e}"

    def create_new_file_and_text(self, file_name: str, text: str) -> str:
        """
        Creates a new file and writes the provided text to it.
        Ensures the directory path exists before creating the file.
        """
        full_path = self._get_full_path(file_name)
        
        # Extract the directory path from the full file path
        directory = os.path.dirname(full_path)
        
        try:
            # Create the directory if it doesn't exist (and any parent directories)
            # exist_ok=True prevents an error if the directory already exists
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True) # This is the key line for cascade creation

            with open(full_path, 'w') as f:
                f.write(text)
            return ""
        except Exception as e:
            return f"An error occurred while creating new file with text: {e}"

    def delete_file(self, file_name: str) -> str:
        """
        Deletes a file from the managed directory.
        """
        full_path = self._get_full_path(file_name)
        try:
            if os.path.exists(full_path):
                os.remove(full_path)
                return ""
            else:
                return f"File '{file_name}' not found."
        except Exception as e:
            return f"An error occurred while deleting file: {e}"

    def list_files(self) -> tuple[list, str]:
        """
        Lists all files in the managed directory.
        """
        try:
            files = [f for f in os.listdir(self.base_dir) if os.path.isfile(self._get_full_path(f))]
            return files, ""
        except Exception as e:
            return [], f"An error occurred while listing files: {e}"

    # --- Folder operations ---
    def create_folder(self, folder_name: str) -> str:
        """
        Creates a new folder within the managed directory.
        """
        full_path = os.path.join(self.base_dir, folder_name)
        try:
            if os.path.exists(full_path):
                return f"Folder '{folder_name}' already exists."
            os.makedirs(full_path)
            return ""
        except Exception as e:
            return f"An error occurred while creating folder: {e}"