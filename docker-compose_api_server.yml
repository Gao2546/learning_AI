# version: '3.8'

# services:

#   api_server:
#     build:
#       context: .
#       dockerfile: api_server/Dockerfile
#     # image: ai_agent_api
#     ports:
#       - "5000:5000"
#     environment:
#       - PGDATABASE=ai_agent #<=========================================
#       - PGUSER=athip
#       - PGPASSWORD=123456
#       - PGHOST=db
#       - PGPORT=5432
#       - IS_DOCKER=true
#       - API_APP=http://app:3000
#       # Pass the OpenAI API key from the host environment or a .env file
#       # Ensure OPENAI_API_KEY is set in your environment before running docker-compose up
#       # - OPENAI_API_KEY=${OPENAI_API_KEY}
#     #---------------------------------------------------------------
#     # deploy:
#     #   resources:
#     #     reservations:
#     #       devices:
#     #         - driver: nvidia
#     #           count: all # Or specify a specific number of GPUs
#     #           capabilities: [gpu]
#     #---------------------------------------------------------------
#     runtime: nvidia
#     networks:
#       - app-network

#     # Deploy constraints to ensure 'api_server' runs on Host B
#     # deploy:
#     #   placement:
#     #     constraints:
#     #       - node.hostname == host_b_hostname # Replace with Host B's actual hostname
#     #    resources: # If you need GPU resource reservation
#     #      reservations:
#     #        devices:
#     #          - driver: nvidia
#     #            count: all # Or specify a specific number of GPUs
#     #            capabilities: [gpu]


#   ollama:
#     image: ollama/ollama
#     # ports:
#       # - "11434:11434"
#     volumes:
#       - /usr/share/ollama/.ollama:/root/.ollama
#     networks:
#       - app-network
#     #-------------------------------------------------------------------
#     # Uncomment below if you want GPU access with NVIDIA
#     # deploy:
#     #   resources:
#     #     reservations:
#     #       devices:
#     #         - driver: nvidia
#     #           count: all
#     #           capabilities: [gpu]
#     #-------------------------------------------------------------------
#     runtime: nvidia

#     # Deploy constraints to ensure 'ollama' runs on Host B
#     # deploy:
#     #   placement:
#     #     constraints:
#     #       - node.hostname == host_b_hostname # Replace with Host B's actual hostname
#     #    resources: # If you need GPU resource reservation
#     #      reservations:
#     #        devices:
#     #          - driver: nvidia
#     #            count: all
#     #            capabilities: [gpu]

# # networks:
# #   app-network:
# #     # driver: bridge
# #     external:
# #       # name: my_shared_overlay_network # <--- Reference the external network
# #       name: my_simulation_network

# networks:
#   app-network:
#     external: true
#     name: my_simulation_network






























version: '3.8'

services:

  api_server:
    build:
      context: .
      dockerfile: api_server/Dockerfile
    # image: ai_agent_api
    ports:
      - "5000:5000"
    environment:
      - PGDATABASE=ai_agent #<=========================================
      - PGUSER=athip
      - PGPASSWORD=123456
      - PGHOST=100.24.67.51
      - PGPORT=5432
      - IS_DOCKER=true
      - API_APP=http://100.24.67.51:3000
      # Pass the OpenAI API key from the host environment or a .env file
      # Ensure OPENAI_API_KEY is set in your environment before running docker-compose up
      # - OPENAI_API_KEY=${OPENAI_API_KEY}
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all # Or specify a specific number of GPUs
    #           capabilities: [gpu]
    #runtime: nvidia
    networks:
      app-network:
        aliases:
          - api_server

    # Deploy constraints to ensure 'api_server' runs on Host B
    # deploy:
    #   placement:
    #     constraints:
    #       - node.hostname == ip-172-31-45-90 # Replace with Host B's actual hostname
    #  resources: # If you need GPU resource reservation
    #    reservations:
    #      devices:
    #        - driver: nvidia
    #          count: all # Or specify a specific number of GPUs
    #          capabilities: [gpu]


  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - /usr/share/ollama/.ollama:/root/.ollama
    networks:
      app-network:
        aliases:
          - api_server

    # Uncomment below if you want GPU access with NVIDIA
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    #runtime: nvidia

    # Deploy constraints to ensure 'ollama' runs on Host B
    # deploy:
    #   placement:
    #     constraints:
    #       - node.hostname == ip-172-31-45-90 # Replace with Host B's actual hostname
     # resources: # If you need GPU resource reservation
     #   reservations:
     #     devices:
     #       - driver: nvidia
     #         count: all
     #         capabilities: [gpu]

networks:
 app-network:
    driver: bridge
#    external:
#       name: my_shared_overlay_network # <--- Reference the external network
      # name: my_simulation_network


# networks:
#   app-network:
#     external: true
#     name: my_shared_overlay_network