version: '3.8'

services:

  api_server:
    build:
      context: .
      dockerfile: api_server/Dockerfile
    # image: ai_agent_api
    ports:
      - "5000:5000"
    environment:
      - PGDATABASE=ai_agent #<=========================================
      - PGUSER=athip
      - PGPASSWORD=123456
      - PGHOST=db
      - PGPORT=5432
      - IS_DOCKER=true
      - API_APP=http://app:3000
      # Pass the OpenAI API key from the host environment or a .env file
      # Ensure OPENAI_API_KEY is set in your environment before running docker-compose up
      # - OPENAI_API_KEY=${OPENAI_API_KEY}
    #---------------------------------------------------------------
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all # Or specify a specific number of GPUs
    #           capabilities: [gpu]
    #---------------------------------------------------------------
    runtime: nvidia
    networks:
      - app-network

    # Deploy constraints to ensure 'api_server' runs on Host B
    # deploy:
    #   placement:
    #     constraints:
    #       - node.hostname == host_b_hostname # Replace with Host B's actual hostname
    #    resources: # If you need GPU resource reservation
    #      reservations:
    #        devices:
    #          - driver: nvidia
    #            count: all # Or specify a specific number of GPUs
    #            capabilities: [gpu]


  ollama:
    image: ollama/ollama
    # ports:
      # - "11434:11434"
    volumes:
      - /usr/share/ollama/.ollama:/root/.ollama
    networks:
      - app-network
    #-------------------------------------------------------------------
    # Uncomment below if you want GPU access with NVIDIA
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    #-------------------------------------------------------------------
    runtime: nvidia

    # Deploy constraints to ensure 'ollama' runs on Host B
    # deploy:
    #   placement:
    #     constraints:
    #       - node.hostname == host_b_hostname # Replace with Host B's actual hostname
    #    resources: # If you need GPU resource reservation
    #      reservations:
    #        devices:
    #          - driver: nvidia
    #            count: all
    #            capabilities: [gpu]

# networks:
#   app-network:
#     # driver: bridge
#     external:
#       # name: my_shared_overlay_network # <--- Reference the external network
#       name: my_simulation_network

networks:
  app-network:
    driver: bridge
    # external: true
    # name: my_simulation_network






























# version: '3.8'

# services:

#   api_server:
#     build:
#       context: .
#       dockerfile: api_server/Dockerfile
#     # image: ai_agent_api
#     ports:
#       - "5000:5000"
#     environment:
#       - PGDATABASE=ai_agent #<=========================================
#       - PGUSER=athip
#       - PGPASSWORD=123456
#       - PGHOST=100.24.67.51
#       - PGPORT=5432
#       - IS_DOCKER=true
#       - API_APP=http://100.24.67.51:3000
#       # Pass the OpenAI API key from the host environment or a .env file
#       # Ensure OPENAI_API_KEY is set in your environment before running docker-compose up
#       # - OPENAI_API_KEY=${OPENAI_API_KEY}
#     # deploy:
#     #   resources:
#     #     reservations:
#     #       devices:
#     #         - driver: nvidia
#     #           count: all # Or specify a specific number of GPUs
#     #           capabilities: [gpu]
#     #runtime: nvidia
#     networks:
#       app-network:
#         aliases:
#           - api_server

#     # Deploy constraints to ensure 'api_server' runs on Host B
#     # deploy:
#     #   placement:
#     #     constraints:
#     #       - node.hostname == ip-172-31-45-90 # Replace with Host B's actual hostname
#     #  resources: # If you need GPU resource reservation
#     #    reservations:
#     #      devices:
#     #        - driver: nvidia
#     #          count: all # Or specify a specific number of GPUs
#     #          capabilities: [gpu]


#   ollama:
#     image: ollama/ollama
#     ports:
#       - "11434:11434"
#     volumes:
#       - /usr/share/ollama/.ollama:/root/.ollama
#     networks:
#       app-network:
#         aliases:
#           - api_server

#     # Uncomment below if you want GPU access with NVIDIA
#     # deploy:
#     #   resources:
#     #     reservations:
#     #       devices:
#     #         - driver: nvidia
#     #           count: all
#     #           capabilities: [gpu]
#     #runtime: nvidia

#     # Deploy constraints to ensure 'ollama' runs on Host B
#     # deploy:
#     #   placement:
#     #     constraints:
#     #       - node.hostname == ip-172-31-45-90 # Replace with Host B's actual hostname
#      # resources: # If you need GPU resource reservation
#      #   reservations:
#      #     devices:
#      #       - driver: nvidia
#      #         count: all
#      #         capabilities: [gpu]

# networks:
#  app-network:
#     driver: bridge
# #    external:
# #       name: my_shared_overlay_network # <--- Reference the external network
#       # name: my_simulation_network


# # networks:
# #   app-network:
# #     external: true
# #     name: my_shared_overlay_network